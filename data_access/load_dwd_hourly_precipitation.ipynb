{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Jupyter notebook for downloading DWD Hourly Precipitation Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earth System Data Processing - Homework 1\n",
    "**Author:** Darya Fomicheva  \n",
    "\n",
    "**Matriculation number:** 7447755\n",
    "\n",
    "**Date:** 04 December 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content of this notebook\n",
    "\n",
    "This notebook demonstrates how to download and process hourly precipitation data for May 2024 from stations in North Rhine-Westphalia, implementing the following steps:\n",
    "\n",
    "- describe the data source (DWD Open Data) and the hourly station precipitation dataset used in this homework\n",
    "- navigate the DWD Open Data directory structure and locate the hourly precipitation files\n",
    "- define the time period of interest (May 2024) and identify the corresponding zip archives\n",
    "- download the selected zip files from the DWD Open Data server\n",
    "- unpack the downloaded archives and load the data into Python (pandas)\n",
    "- perform basic checks to verify that the download has been successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source: Deutscher Wetterdienst (DWD)\n",
    "\n",
    "The data used in this homework are obtained from the Deutscher Wetterdienst (DWD), the national meteorological service of Germany. DWD has a legal mandate to make most of its weather and climate information publicly available and fulfils this mandate by operating an official **Open Data server** at `https://opendata.dwd.de`.  \n",
    "\n",
    "On this server, DWD offers weather and climate data free of charge within its legal mandate. Access is provided directly via HTTPS and does not require registration. Users can download files anonymously under the terms of use specified by DWD (copyright and usage conditions). This Open Data area includes a large range of climate datasets from the DWD Climate Data Center (CDC). These datasets are available for direct download via HTTP and FTP.\n",
    "\n",
    "For this homework, I selected the DWD dataset with hourly precipitation observations from meteorological stations across Germany available on this Open Data service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset overview\n",
    "The observations in this dataset come from stations operated by DWD as well as from legally and qualitatively equivalent partner networks. For each station, extensive metadata are provided, including information on station relocations, instrument changes, changes in reference times, processing algorithms and operator details.\n",
    "\n",
    "According to the official dataset description provided by the DWD Climate Data Center, the hourly precipitation dataset has the following main characteristics:\n",
    "\n",
    "- **Name:** Hourly station observations of precipitation for Germany (version v24.03)  \n",
    "- **Provider:** DWD Climate Data Center (CDC)  \n",
    "- **Parameters:** hourly precipitation height and related precipitation variables, including indicators of whether precipitation fell, the kind and form of precipitation, and associated quality and indicator flags  \n",
    "- **Unit:** millimetres (mm)  \n",
    "- **Statistical processing:** time series of hourly sums  \n",
    "- **Temporal coverage:** from 1995-09-01 onwards  \n",
    "- **Spatial coverage:** observation stations distributed across Germany  \n",
    "- **Access URL:** `https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/`  \n",
    "\n",
    "The dataset is organised into a versioned archive (`historical/`) and a daily updated part covering roughly the last 500 days (`recent/`). The `historical/` directory is updated about once per year and its contents remain stable, whereas data in `recent/` are still undergoing quality control and may change as further checks and corrections are applied.\n",
    "\n",
    "In this notebook, I use only station files from the `historical/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying files covering May 2024\n",
    "\n",
    "In the homework description, the time period 1–31 May 2024 is specified as the target interval for the download. In the DWD hourly precipitation dataset, each station is stored in a separate zip archive in the `historical/` directory. The file names follow the pattern\n",
    "\n",
    "`stundenwerte_RR_{station_id}_{begin_date}_{end_date}_hist.zip`\n",
    "\n",
    "where `RR` is the product code for hourly precipitation, `station_id` is the numeric station identifier, and `begin_date` and `end_date` define the time period covered by the file (format `YYYYMMDD`). To obtain all data for May 2024, the station archives whose coverage interval `[begin_date, end_date]` includes the target period `[20240501, 20240531]` have to be selected. \n",
    "\n",
    "The mapping between `station_id` and station metadata such as station name, location and height is provided in the file `RR_Stundenwerte_Beschreibung_Stationen.txt` in the same directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting the number of archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RR zip files in 'historical/': 1453\n"
     ]
    }
   ],
   "source": [
    "# 1) Basic setup: where to look, and what time period we want\n",
    "\n",
    "BASE_URL = (\n",
    "    \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/\"\n",
    "    \"climate/hourly/precipitation/historical/\"\n",
    ")\n",
    "\n",
    "# Target period: May 2024 (inclusive)\n",
    "MAY_START = datetime(2024, 5, 1)\n",
    "MAY_END   = datetime(2024, 5, 31)\n",
    "\n",
    "# 2) Download the directory listing (HTML) from the DWD server\n",
    "\n",
    "resp = requests.get(BASE_URL)\n",
    "resp.raise_for_status()\n",
    "html = resp.text\n",
    "\n",
    "# 3) Find all ZIP filenames + extract their begin/end dates\n",
    "\n",
    "zip_pattern = re.compile(\n",
    "    r'href=\"(stundenwerte_RR_\\d+_(\\d{8})_(\\d{8})_hist\\.zip)\"'\n",
    ")\n",
    "\n",
    "matches = zip_pattern.findall(html)\n",
    "print(\"Total RR zip files in 'historical/':\", len(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zip files covering May 2024: 1335\n",
      "First 5 matching filenames:\n",
      " - stundenwerte_RR_00020_20040814_20241231_hist.zip\n",
      " - stundenwerte_RR_00029_20060110_20241231_hist.zip\n",
      " - stundenwerte_RR_00044_20070401_20241231_hist.zip\n",
      " - stundenwerte_RR_00046_20060101_20241231_hist.zip\n",
      " - stundenwerte_RR_00053_20051001_20241231_hist.zip\n"
     ]
    }
   ],
   "source": [
    "# 4) Keep only files whose date range overlaps with May 2024\n",
    "\n",
    "\n",
    "relevant_files = []\n",
    "\n",
    "for filename, begin_str, end_str in matches:\n",
    "    begin_date = datetime.strptime(begin_str, \"%Y%m%d\")\n",
    "    end_date   = datetime.strptime(end_str, \"%Y%m%d\")\n",
    "\n",
    "    # We keep files whose coverage interval [begin_date, end_date]\n",
    "    # overlaps with the target period [MAY_START, MAY_END]\n",
    "    overlaps = (begin_date <= MAY_END) and (end_date >= MAY_START)\n",
    "    if overlaps:\n",
    "        relevant_files.append(filename)\n",
    "\n",
    "print(\"Number of zip files covering May 2024:\", len(relevant_files))\n",
    "\n",
    "\n",
    "# 5) Print a few examples (sanity check)\n",
    "\n",
    "print(\"First 5 matching filenames:\")\n",
    "for fn in relevant_files[:5]:\n",
    "    print(\" -\", fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, 1453 station archives are available in the `historical/` directory. Of these, 1335 contain data for May 2024. This is already a very large number of stations for just one month and one variable, and it means that I would almost download the entire historical hourly precipitation dataset for Germany. Even though the total download volume (about 0.6 GB) is still manageable on a standard laptop, this is not very convenient when the actual interest is only a single month. To make the dataset more manageable, I decided to restrict the selection to stations from one federal state, namely North Rhine-Westphalia. This still provides an interesting regional dataset, but with fewer station archives and a smaller data volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection of NRW Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stations_id von_datum bis_datum Stationshoehe geoBreite geoLaenge Stationsname Bundesland Abgabe\n",
      "----------- --------- --------- ------------- --------- --------- ----------------------------------------- ---------- ------\n",
      "00003 19950901 20110401            202     50.7827    6.0941 Aachen                                   Nordrhein-Westfalen                      Frei                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "00020 20040814 20251204            432     48.9219    9.9129 Abtsgmünd-Untergröningen                 Baden-Württemberg                        Frei                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "00029 20060110 20251204            260     49.7175   10.9101 Adelsdorf (Kläranlage)                   Bayern                                   Frei                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n"
     ]
    }
   ],
   "source": [
    "metadata_url = (\n",
    "    \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/\"\n",
    "    \"climate/hourly/precipitation/historical/RR_Stundenwerte_Beschreibung_Stationen.txt\"\n",
    ")\n",
    "\n",
    "#1) Download the metadata file as plain text\n",
    "\n",
    "response = requests.get(metadata_url)\n",
    "response.raise_for_status()\n",
    "text = response.text\n",
    "\n",
    "lines = text.splitlines()\n",
    "\n",
    "# Optional: look at the first few lines\n",
    "for line in lines[:5]:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The station metadata is provided as a fixed-width text file instead of a CSV table, which means that extracting specific columns (e.g. Bundesland) involves an additional parsing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations in North Rhine-Westphalia: 199\n"
     ]
    }
   ],
   "source": [
    "metadata_url = (\n",
    "    \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/\"\n",
    "    \"climate/hourly/precipitation/historical/\"\n",
    "    \"RR_Stundenwerte_Beschreibung_Stationen.txt\"\n",
    ")\n",
    "\n",
    "# 1) Download the metadata file as plain text\n",
    "\n",
    "response = requests.get(metadata_url)\n",
    "response.raise_for_status()\n",
    "text = response.text\n",
    "\n",
    "lines = text.splitlines()\n",
    "\n",
    "# 2) Go through all lines and collect station IDs for NRW\n",
    "#    Rule: if the 8th column == 'Nordrhein-Westfalen',\n",
    "#          keep the ID from the 1st column.\n",
    "\n",
    "nrw_ids = []\n",
    "\n",
    "for line in lines:\n",
    "    # Skip empty lines\n",
    "    if not line.strip():\n",
    "        continue\n",
    "\n",
    "    # Skip header line that starts with 'Stations_id'\n",
    "    if line.lstrip().startswith(\"Stations_id\"):\n",
    "        continue\n",
    "\n",
    "    # Split by any whitespace into \"columns\"\n",
    "    parts = line.split()\n",
    "\n",
    "    # We need at least 8 columns to access parts[7]\n",
    "    if len(parts) < 8:\n",
    "        continue\n",
    "\n",
    "    station_id_str = parts[0]\n",
    "    bundesland = parts[7]\n",
    "\n",
    "    if bundesland == \"Nordrhein-Westfalen\":\n",
    "        # Convert to int (optional, can also keep as string)\n",
    "        try:\n",
    "            nrw_ids.append(int(station_id_str))\n",
    "        except ValueError:\n",
    "            # If parsing fails, just skip this line\n",
    "            continue\n",
    "\n",
    "# Remove duplicates and sort\n",
    "nrw_ids = sorted(set(nrw_ids))\n",
    "\n",
    "print(f\"Number of stations in North Rhine-Westphalia: {len(nrw_ids)}\")\n",
    "\n",
    "# 3) If you want a set of IDs as strings (for matching filenames)\n",
    "\n",
    "nrw_id_set = set(str(sid) for sid in nrw_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, the directory contained 199 ZIP archives corresponding to stations located in North Rhine-Westphalia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RR zip files in 'historical/': 1453\n",
      "Number of NRW station archives covering May 2024: 73\n",
      "[download] stundenwerte_RR_00216_20041001_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_00555_20080101_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_00603_19990303_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_00613_20041101_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_00644_20050101_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_00796_20041101_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_00871_20050801_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_00902_20061001_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_00934_20041001_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_00989_20050201_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_01024_20060801_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_01046_20041001_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_01078_19950901_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_01241_20061201_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_01246_20150801_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_01300_20040601_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_01303_19950901_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_01327_20040801_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_01590_20030701_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_01595_20121001_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_01766_19950901_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_02027_20060601_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_02110_20030105_20241201_hist.zip ... done\n",
      "[download] stundenwerte_RR_02254_20050601_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_02497_20040801_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_02629_20040701_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_02667_19950901_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_02947_20061001_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_02968_20081201_20240902_hist.zip ... done\n",
      "[download] stundenwerte_RR_02999_20040701_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_03031_20040701_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_03081_20071201_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_03098_19950901_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_03215_20070601_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_03321_20050701_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_03339_20060901_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_03499_20060701_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_03540_20041101_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_03591_20040601_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_03795_20041201_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_03913_20040701_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_04063_20030701_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_04127_20050101_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_04150_20051201_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_04313_20040801_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_04368_20041001_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_04488_20060801_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_04741_20041201_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_04849_20050601_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_05064_20041201_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_05347_19950901_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_05360_20070701_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_05480_20030910_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_05513_20050901_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_05619_20041201_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_05699_20041101_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_05717_20060901_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_05733_20050501_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_06197_20001013_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_06264_20040601_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_06313_20041201_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_06337_20040801_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_07106_20060901_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_07330_20051001_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_07344_20060601_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_07374_20060301_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_13669_20070901_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_13670_20070601_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_13671_20071201_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_13696_20071201_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_13700_20080501_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_13713_20071101_20241231_hist.zip ... done\n",
      "[download] stundenwerte_RR_15000_20110401_20241231_hist.zip ... done\n",
      "Download step finished.\n"
     ]
    }
   ],
   "source": [
    "# 0) Prepare NRW station ID set in the same format as filenames\n",
    "\n",
    "nrw_id_set = set(f\"{sid:05d}\" for sid in nrw_ids) \n",
    "\n",
    "\n",
    "# 1) Base URL and local download folder\n",
    "\n",
    "BASE_URL = (\n",
    "    \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/\"\n",
    "    \"climate/hourly/precipitation/historical/\"\n",
    ")\n",
    "\n",
    "DATA_DIR = Path(\"data/dwd_rr_hourly\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2) Get the directory listing from DWD and find all RR zip files\n",
    "\n",
    "resp = requests.get(BASE_URL)\n",
    "resp.raise_for_status()\n",
    "html = resp.text\n",
    "\n",
    "# Example filename:\n",
    "# stundenwerte_RR_00001_19950901_20241231_hist.zip\n",
    "pattern = re.compile(\n",
    "    r'href=\"(stundenwerte_RR_(\\d{5})_(\\d{8})_(\\d{8})_hist\\.zip)\"'\n",
    ")\n",
    "\n",
    "matches = pattern.findall(html)\n",
    "print(\"Total RR zip files in 'historical/':\", len(matches))\n",
    "\n",
    "\n",
    "# 3) Filter files: (a) station in NRW, (b) overlap with May 2024\n",
    "\n",
    "MAY_START = datetime(2024, 5, 1)\n",
    "MAY_END   = datetime(2024, 5, 31)\n",
    "\n",
    "nrw_may_files = []\n",
    "\n",
    "for filename, station_id, begin_str, end_str in matches:\n",
    "    # keep only NRW stations\n",
    "    if station_id not in nrw_id_set:\n",
    "        continue\n",
    "\n",
    "    begin_date = datetime.strptime(begin_str, \"%Y%m%d\")\n",
    "    end_date   = datetime.strptime(end_str, \"%Y%m%d\")\n",
    "\n",
    "    # overlap condition: [begin_date, end_date] ∩ [MAY_START, MAY_END] ≠ ∅\n",
    "    overlaps_may = (begin_date <= MAY_END) and (end_date >= MAY_START)\n",
    "    if not overlaps_may:\n",
    "        continue\n",
    "\n",
    "    nrw_may_files.append(filename)\n",
    "\n",
    "print(\"Number of NRW station archives covering May 2024:\", len(nrw_may_files))\n",
    "\n",
    "\n",
    "# 4) Helper function: download one ZIP file\n",
    "\n",
    "def download_zip_file(url: str, local_path: Path) -> None:\n",
    "    \"\"\"Download a ZIP file from url and save it to local_path.\"\"\"\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "\n",
    "# 5) Main loop: download only the NRW+May 2024 files\n",
    "\n",
    "for filename in nrw_may_files:\n",
    "    url = BASE_URL + filename\n",
    "    local_path = DATA_DIR / filename\n",
    "\n",
    "    # Skip if the file is already present locally\n",
    "    if local_path.exists():\n",
    "        print(f\"[skip] {filename} (already exists)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[download] {filename} ...\", end=\" \")\n",
    "    try:\n",
    "        download_zip_file(url, local_path)\n",
    "        print(\"done\")\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED ({e})\")\n",
    "\n",
    "print(\"Download step finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, 73 station archives were downloaded. On my local machine, this took about one minute and required roughly 36 MB of disk space, which is easily manageable on a standard laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)  Pick one ZIP file to inspect\n",
    "\n",
    "DATA_DIR = Path(\"data/dwd_rr_hourly\")\n",
    "\n",
    "zip_path = DATA_DIR / \"stundenwerte_RR_00216_20041001_20241231_hist.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files inside the zip:\n",
      " - Metadaten_Stationsname_Betreibername_00216.html\n",
      " - Metadaten_Stationsname_Betreibername_00216.txt\n",
      " - Metadaten_Parameter_rr_stunde_00216.html\n",
      " - Metadaten_Parameter_rr_stunde_00216.txt\n",
      " - Metadaten_Geraete_Niederschlagshoehe_00216.html\n",
      " - Metadaten_Geraete_Niederschlagshoehe_00216.txt\n",
      " - Metadaten_Geographie_00216.txt\n",
      " - Metadaten_Fehldaten_00216_20041001_20241231.html\n",
      " - Metadaten_Fehldaten_00216_20041001_20241231.txt\n",
      " - Metadaten_Fehlwerte_00216_20041001_20241231.txt\n",
      " - produkt_rr_stunde_20041001_20241231_00216.txt\n"
     ]
    }
   ],
   "source": [
    "# 2) Open the ZIP and list all files inside (just to see what's there)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "    names = zf.namelist()\n",
    "\n",
    "    print(\"\\nFiles inside the zip:\")\n",
    "    for name in names:\n",
    "        print(\" -\", name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Files inside the zip:\n",
      " - Metadaten_Stationsname_Betreibername_00216.html\n",
      " - Metadaten_Stationsname_Betreibername_00216.txt\n",
      " - Metadaten_Parameter_rr_stunde_00216.html\n",
      " - Metadaten_Parameter_rr_stunde_00216.txt\n",
      " - Metadaten_Geraete_Niederschlagshoehe_00216.html\n",
      " - Metadaten_Geraete_Niederschlagshoehe_00216.txt\n",
      " - Metadaten_Geographie_00216.txt\n",
      " - Metadaten_Fehldaten_00216_20041001_20241231.html\n",
      " - Metadaten_Fehldaten_00216_20041001_20241231.txt\n",
      " - Metadaten_Fehlwerte_00216_20041001_20241231.txt\n",
      " - produkt_rr_stunde_20041001_20241231_00216.txt\n",
      "\n",
      "Product file: produkt_rr_stunde_20041001_20241231_00216.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATIONS_ID</th>\n",
       "      <th>MESS_DATUM</th>\n",
       "      <th>QN_8</th>\n",
       "      <th>R1</th>\n",
       "      <th>RS_IND</th>\n",
       "      <th>WRTR</th>\n",
       "      <th>eor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216</td>\n",
       "      <td>2004100100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>2004100101</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>eor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>2004100102</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>eor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>2004100103</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>2004100104</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>eor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATIONS_ID  MESS_DATUM  QN_8    R1  RS_IND  WRTR  eor\n",
       "0          216  2004100100     1   0.0     0.0   NaN  eor\n",
       "1          216  2004100101     1   0.0     0.0   0.0  eor\n",
       "2          216  2004100102     1   0.0     0.0   0.0  eor\n",
       "3          216  2004100103     1   0.0     0.0   NaN  eor\n",
       "4          216  2004100104     1   0.0     0.0   0.0  eor"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 3) Open ZIP, find product file, read it into a DataFrame\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "    names = zf.namelist()\n",
    "\n",
    "    print(\"\\nFiles inside the zip:\")\n",
    "    for name in names:\n",
    "        print(\" -\", name)\n",
    "\n",
    "    # Find the produkt*.txt file (usually only one)\n",
    "    product_name = None\n",
    "    for n in names:\n",
    "        lower = n.lower()\n",
    "        if (\"produkt\" in lower) and lower.endswith(\".txt\"):\n",
    "            product_name = n\n",
    "            break\n",
    "\n",
    "    if product_name is None:\n",
    "        raise ValueError(\"No produkt*.txt file found inside the zip archive.\")\n",
    "\n",
    "    print(\"\\nProduct file:\", product_name)\n",
    "\n",
    "    # Read that product file directly from the ZIP into a pandas DataFrame\n",
    "    with zf.open(product_name) as f:\n",
    "        df_00216 = pd.read_csv(\n",
    "            f,\n",
    "            sep=\";\",                \n",
    "            na_values=[-999, -999.0] \n",
    "        )\n",
    "df_00216.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting May 2024 from one station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DWD hourly precipitation files, the timestamp is stored in the column MESS_DATUM. It is encoded as YYYYMMDDHH (year, month, day, hour). In this step I convert MESS_DATUM to a proper datetime column and then filter the time series to keep only observations from May 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATIONS_ID</th>\n",
       "      <th>MESS_DATUM</th>\n",
       "      <th>QN_8</th>\n",
       "      <th>R1</th>\n",
       "      <th>RS_IND</th>\n",
       "      <th>WRTR</th>\n",
       "      <th>eor</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>171159</th>\n",
       "      <td>216</td>\n",
       "      <td>2024050100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eor</td>\n",
       "      <td>2024-05-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171160</th>\n",
       "      <td>216</td>\n",
       "      <td>2024050101</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eor</td>\n",
       "      <td>2024-05-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171161</th>\n",
       "      <td>216</td>\n",
       "      <td>2024050102</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eor</td>\n",
       "      <td>2024-05-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171162</th>\n",
       "      <td>216</td>\n",
       "      <td>2024050103</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eor</td>\n",
       "      <td>2024-05-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171163</th>\n",
       "      <td>216</td>\n",
       "      <td>2024050104</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eor</td>\n",
       "      <td>2024-05-01 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        STATIONS_ID  MESS_DATUM  QN_8    R1  RS_IND  WRTR  eor  \\\n",
       "171159          216  2024050100     3   0.0     0.0   NaN  eor   \n",
       "171160          216  2024050101     3   0.0     0.0   NaN  eor   \n",
       "171161          216  2024050102     3   0.0     0.0   NaN  eor   \n",
       "171162          216  2024050103     3   0.0     0.0   NaN  eor   \n",
       "171163          216  2024050104     3   0.0     0.0   NaN  eor   \n",
       "\n",
       "                  datetime  \n",
       "171159 2024-05-01 00:00:00  \n",
       "171160 2024-05-01 01:00:00  \n",
       "171161 2024-05-01 02:00:00  \n",
       "171162 2024-05-01 03:00:00  \n",
       "171163 2024-05-01 04:00:00  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Convert MESS_DATUM (YYYYMMDDHH) to pandas datetime\n",
    "\n",
    "ts = df_00216[\"MESS_DATUM\"].astype(str).str.strip()\n",
    "ts = ts.str.replace(r\"\\.0$\", \"\", regex=True) \n",
    "\n",
    "df_00216[\"datetime\"] = pd.to_datetime(ts, format=\"%Y%m%d%H\", errors=\"coerce\")\n",
    "\n",
    "# 2) Filter to May 2024\n",
    "\n",
    "may_start = pd.Timestamp(\"2024-05-01\")\n",
    "june_start = pd.Timestamp(\"2024-06-01\")\n",
    "\n",
    "mask_may = (df_00216[\"datetime\"] >= may_start) & (df_00216[\"datetime\"] < june_start)\n",
    "df_00216_may2024 = df_00216.loc[mask_may].copy()\n",
    "\n",
    "# 3) Quick sanity checks\n",
    "\n",
    "df_00216_may2024 = df_00216_may2024.sort_values(\"datetime\")\n",
    "df_00216_may2024.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting May 2024 and joining all stations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATIONS_ID</th>\n",
       "      <th>MESS_DATUM</th>\n",
       "      <th>QN_8</th>\n",
       "      <th>R1</th>\n",
       "      <th>RS_IND</th>\n",
       "      <th>WRTR</th>\n",
       "      <th>eor</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>216</td>\n",
       "      <td>2024050100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eor</td>\n",
       "      <td>2024-05-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>216</td>\n",
       "      <td>2024050101</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eor</td>\n",
       "      <td>2024-05-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>2024050102</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eor</td>\n",
       "      <td>2024-05-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>216</td>\n",
       "      <td>2024050103</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eor</td>\n",
       "      <td>2024-05-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>216</td>\n",
       "      <td>2024050104</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eor</td>\n",
       "      <td>2024-05-01 04:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STATIONS_ID  MESS_DATUM  QN_8    R1  RS_IND  WRTR  eor            datetime\n",
       "0          216  2024050100     3   0.0     0.0   NaN  eor 2024-05-01 00:00:00\n",
       "1          216  2024050101     3   0.0     0.0   NaN  eor 2024-05-01 01:00:00\n",
       "2          216  2024050102     3   0.0     0.0   NaN  eor 2024-05-01 02:00:00\n",
       "3          216  2024050103     3   0.0     0.0   NaN  eor 2024-05-01 03:00:00\n",
       "4          216  2024050104     3   0.0     0.0   NaN  eor 2024-05-01 04:00:00"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic setup: folder with NRW ZIP files + time window\n",
    "\n",
    "DATA_DIR = Path(\"data/dwd_rr_hourly\")\n",
    "\n",
    "may_start = pd.Timestamp(\"2024-05-01\")\n",
    "june_start = pd.Timestamp(\"2024-06-01\") \n",
    "\n",
    "nrw_dfs = []  # here we collect May 2024 data from all NRW stations\n",
    "\n",
    "# 2) Loop over all ZIP files in the folder\n",
    "\n",
    "for zip_path in sorted(DATA_DIR.glob(\"*.zip\")):\n",
    "\n",
    "    # DWD filename pattern:\n",
    "    # stundenwerte_RR_<STATIONS_ID>_<BEGIN>_<END>_hist.zip\n",
    "    parts = zip_path.name.split(\"_\")\n",
    "    if len(parts) < 4:\n",
    "        continue\n",
    "\n",
    "    station_id_str = parts[2] \n",
    "\n",
    "    \n",
    "    # 3) Open ZIP and find the produkt*.txt file\n",
    "    \n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "            names = zf.namelist()\n",
    "\n",
    "            product_name = None\n",
    "            for n in names:\n",
    "                lower = n.lower()\n",
    "                if (\"produkt\" in lower) and lower.endswith(\".txt\"):\n",
    "                    product_name = n\n",
    "                    break\n",
    "\n",
    "            if product_name is None:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            # 4) Read produkt file into a DataFrame\n",
    "            \n",
    "            with zf.open(product_name) as f:\n",
    "                df = pd.read_csv(\n",
    "                    f,\n",
    "                    sep=\";\",                    \n",
    "                    na_values=[-999, -999.0],    \n",
    "                    encoding=\"latin1\"\n",
    "                )\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    # 5) Convert MESS_DATUM (YYYYMMDDHH) to pandas datetime\n",
    "    \n",
    "    if \"MESS_DATUM\" not in df.columns:\n",
    "        continue\n",
    "\n",
    "    ts = df[\"MESS_DATUM\"].astype(str).str.strip()\n",
    "    ts = ts.str.replace(r\"\\.0$\", \"\", regex=True)  # just in case it came as '....0'\n",
    "\n",
    "    df[\"datetime\"] = pd.to_datetime(\n",
    "        ts,\n",
    "        format=\"%Y%m%d%H\",\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    # 6) Filter rows to May 2024\n",
    "    \n",
    "    mask_may = (df[\"datetime\"] >= may_start) & (df[\"datetime\"] < june_start)\n",
    "    df_may = df.loc[mask_may].copy()\n",
    "\n",
    "    if df_may.empty:\n",
    "        # This station has no data in May 2024\n",
    "        continue\n",
    "\n",
    "    # Sort by time (nice for checks and later analysis)\n",
    "    df_may = df_may.sort_values(\"datetime\")\n",
    "\n",
    "    \n",
    "    # 7) Add station ID and store the filtered DataFrame\n",
    "    \n",
    "    df_may[\"STATIONS_ID\"] = int(station_id_str) \n",
    "\n",
    "    nrw_dfs.append(df_may)\n",
    "\n",
    "\n",
    "\n",
    "# 8) Combine all NRW stations into one DataFrame\n",
    "\n",
    "if nrw_dfs:\n",
    "    nrw_may_2024 = pd.concat(nrw_dfs, ignore_index=True)\n",
    "else:\n",
    "    nrw_may_2024 = pd.DataFrame()\n",
    "\n",
    "nrw_may_2024.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       STATIONS_ID  MESS_DATUM  QN_8    R1  RS_IND  WRTR  eor  \\\n",
      "0              216  2024050100     3   0.0     0.0   NaN  eor   \n",
      "1              216  2024050101     3   0.0     0.0   NaN  eor   \n",
      "2              216  2024050102     3   0.0     0.0   NaN  eor   \n",
      "3              216  2024050103     3   0.0     0.0   NaN  eor   \n",
      "4              216  2024050104     3   0.0     0.0   NaN  eor   \n",
      "...            ...         ...   ...   ...     ...   ...  ...   \n",
      "54008        15000  2024053119     3   0.0     0.0   0.0  eor   \n",
      "54009        15000  2024053120     3   0.4     1.0   6.0  eor   \n",
      "54010        15000  2024053121     3   0.0     0.0   0.0  eor   \n",
      "54011        15000  2024053122     3   0.0     0.0   0.0  eor   \n",
      "54012        15000  2024053123     3   1.2     1.0   6.0  eor   \n",
      "\n",
      "                 datetime  \n",
      "0     2024-05-01 00:00:00  \n",
      "1     2024-05-01 01:00:00  \n",
      "2     2024-05-01 02:00:00  \n",
      "3     2024-05-01 03:00:00  \n",
      "4     2024-05-01 04:00:00  \n",
      "...                   ...  \n",
      "54008 2024-05-31 19:00:00  \n",
      "54009 2024-05-31 20:00:00  \n",
      "54010 2024-05-31 21:00:00  \n",
      "54011 2024-05-31 22:00:00  \n",
      "54012 2024-05-31 23:00:00  \n",
      "\n",
      "[54013 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(nrw_may_2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 54013 entries, 0 to 54012\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   STATIONS_ID  54013 non-null  int64         \n",
      " 1   MESS_DATUM   54013 non-null  int64         \n",
      " 2   QN_8         54013 non-null  int64         \n",
      " 3     R1         53136 non-null  float64       \n",
      " 4   RS_IND       53136 non-null  float64       \n",
      " 5   WRTR         8052 non-null   float64       \n",
      " 6   eor          54013 non-null  object        \n",
      " 7   datetime     54013 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(3), int64(3), object(1)\n",
      "memory usage: 3.3+ MB\n"
     ]
    }
   ],
   "source": [
    "nrw_may_2024.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the dataset is sufficiently compact and structured to allow further analysis and to generate the visualisations of interest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
