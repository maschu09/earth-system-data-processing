{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef487390-5a23-4274-8c8f-ea421dbf6d65",
   "metadata": {},
   "source": [
    "# Homework Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "879322dd-acba-447d-aedb-86ee691ea8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import cdsapi\n",
    "import xarray as xr\n",
    "import hpgeom as hpg\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766aeba-52a8-46cc-a336-52c41bba5f11",
   "metadata": {},
   "source": [
    "### Check necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "c74313b3-8906-497f-8fc6-ac433ead763d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import zarr\n",
    "except ImportError:\n",
    "    !pip install zarr\n",
    "    import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "d0c1aff0-2eae-4c13-9275-d28cb54eb70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import hpgeom as hpg\n",
    "except ImportError:\n",
    "    !pip install hpgeom\n",
    "    import hpgeom as hpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "bc865c16-4646-4504-b022-c9ee7104886c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import cdsapi\n",
    "except ImportError:\n",
    "    !pip install cdsapi\n",
    "    import cdsapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e90052-8c51-4fe8-acad-faa33836f8b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1 - Core control flow & mock-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3f17e-1c5d-4c93-b986-4d3718d1c860",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Date Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "39ce8c06-dde3-47f3-b2ef-944028e74b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generator function to find and store all dates inside of the given range, inclusive the start and end date \n",
    "def daterange(start, end):\n",
    "    \n",
    "    current = start\n",
    "    while current <= end:\n",
    "        yield current\n",
    "        current += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57562249-f1b5-495f-b20a-bbb1dabcb48f",
   "metadata": {},
   "source": [
    "### Status Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "5109d777-de6c-4dc6-a39c-c72412f821ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If necessary, a new directory will be created\n",
    "BASE_DIR = Path(\"processed_data\")\n",
    "BASE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Checks whether a date has already been processed\n",
    "def is_day_processed(day: date) -> bool:\n",
    "    # A date is considered processed if a folder exists for it\n",
    "    return (BASE_DIR / day.isoformat()).exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068e2ef-53d7-4589-b984-e469b5d42b6d",
   "metadata": {},
   "source": [
    "### Mock-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d690175-bfa4-4acb-929a-5780de72007c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mock-processing that simulates download, processing and archiving of daily files\n",
    "def mock_process_day(day, fail_probability=0.2):\n",
    "    print(f\"→ Processing {day}\")\n",
    "\n",
    "    # Simulating a random error during download\n",
    "    if random.random() < fail_probability:\n",
    "        raise RuntimeError(\"Mock download failed\")\n",
    "\n",
    "    # Save the daily mock file\n",
    "    day_dir = BASE_DIR / day.isoformat()\n",
    "    day_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(day_dir / \"done.txt\", \"w\") as f:\n",
    "        f.write(f\"Processed on {datetime.now()}\\n\")\n",
    "\n",
    "    print(f\"✓ {day} successfully processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2559f1bf-6513-4503-994d-49427991b130",
   "metadata": {},
   "source": [
    "### Find oldest missing day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e13fc0a0-b044-48e5-8d84-ddc18bae9457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the oldest missing day of a given date range \n",
    "def find_oldest_missing_day(start, end) -> date | None:\n",
    "    for d in daterange(start, end):\n",
    "        # Use the established logic\n",
    "        if not is_day_processed(d):\n",
    "            return d\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1201cef-24e3-429e-9c11-a5222195ff03",
   "metadata": {},
   "source": [
    "### Find oldest processed day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c68463b-d346-4e90-88c4-457c1267388e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the oldest fully processed day in the directory\n",
    "def find_oldest_processed_day(processed_dir, date_format):\n",
    "    \n",
    "    oldest_date = None\n",
    "    \n",
    "    for entry in os.listdir(processed_dir):\n",
    "        folder_path = os.path.join(processed_dir, entry)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue  # Skip files, just looking for folders\n",
    "        \n",
    "        try:\n",
    "            folder_date = datetime.strptime(entry, date_format).date() # Extract date from folder name\n",
    "        except ValueError:\n",
    "            # Ignore files that do not match the given date format\n",
    "            continue\n",
    "\n",
    "        # Check whether “Done.txt” exists in the folder\n",
    "        done_file = os.path.join(folder_path, \"Done.txt\")\n",
    "        if not os.path.isfile(done_file):\n",
    "            continue  # Date has not been fully processed\n",
    "            \n",
    "        if oldest_date is None or folder_date < oldest_date:\n",
    "            oldest_date = folder_date\n",
    "\n",
    "    return oldest_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b296a33-96a1-4992-b6b9-afd02150b7e0",
   "metadata": {},
   "source": [
    "### Check dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "432e3b97-81ff-4b8c-ba3e-baa681fcd69d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_dates(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    target_date = None\n",
    "):\n",
    "    if end_date and start_date is not None:\n",
    "        if end_date < start_date:\n",
    "            raise ValueError(\"✗ End_date is before start_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339679a-458d-4343-b515-05aa63ad45cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Central Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "2ed5a70f-2f52-40fe-be18-8322917f9f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Central control flow that handles the daily data\n",
    "def run_daily_workflow(\n",
    "    start_date = None,\n",
    "    end_date = None,\n",
    "    target_date = None, \n",
    "    # ensure proper functionaility when script is executed w/o arguments\n",
    "):\n",
    "\n",
    "    # If the function is called without start_date, find a suitable start date\n",
    "    if start_date is None:\n",
    "        start_date = find_oldest_processed_day(BASE_DIR, \"%Y-%m-%d\")\n",
    "    \n",
    "    # If the function is called without end_date, find a suitable end date\n",
    "    if end_date is None:\n",
    "        end_date = date.today()\n",
    "    \n",
    "    # Check that the given date range is correct\n",
    "    check_dates(start_date, end_date)\n",
    "    \n",
    "    # Check whether a target date or all days within the range should be processed\n",
    "    if target_date is not None:\n",
    "            \n",
    "        print(f\"Start workflow for individual target date: {target_date}\")\n",
    "        try:\n",
    "            mock_process_day(target_date)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {target_date}: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Start workflow for all missing days\")\n",
    "\n",
    "    while True:\n",
    "        missing_day = find_oldest_missing_day(start_date, end_date)\n",
    "        if missing_day is None:\n",
    "            print(\"✓ All days are processed\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            mock_process_day(missing_day)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {missing_day}: {e}\")\n",
    "            print(\"→ Error logged, next day will be processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c20ef-92c3-47b3-b414-122925f76081",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2 - Flexible ERA5 donwload routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3381c004-038c-47fd-86e7-765048e7789d",
   "metadata": {},
   "source": [
    "### ERA5 configuration dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f7363308-f7e6-46e5-b1eb-9702606e3c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This dictionary is used to easily adapt the donwload routine to other parameter values as stated in the assignment.\n",
    "# No hardcoded settings within the download routine of the core workflow  \n",
    "\n",
    "ERA5_CONFIG = {\n",
    "    \"dataset\": \"reanalysis-era5-pressure-levels\",\n",
    "    \"format\": \"netcdf\",\n",
    "    \"variable\": [\"specific_humidity\"],\n",
    "    \"pressure_levels\": [975, 900, 800, 500, 300],\n",
    "    \"times\": [\"00:00\", \"06:00\", \"12:00\", \"18:00\"],\n",
    "    \"filename\": \"era5_humidity.nc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2779b3-97f3-4b98-bd19-80d30d816436",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download ERA5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d04932d0-cfc6-497a-b187-c4aea7699eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downloads ERA5 humidity data for a single given day\n",
    "def download_era5_humidity(day):\n",
    "\n",
    "    # Save the daily data\n",
    "    day_dir = BASE_DIR / day.isoformat()\n",
    "    day_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    target_file = day_dir / \"era5_humidity.nc\"\n",
    "\n",
    "    # CDS API request\n",
    "    c = cdsapi.Client()\n",
    "    request = {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": ERA5_CONFIG[\"variable\"],\n",
    "        \"pressure_level\": [str(p) for p in ERA5_CONFIG[\"pressure_levels\"]],\n",
    "        \"year\": day.strftime(\"%Y\"),\n",
    "        \"month\": day.strftime(\"%m\"),\n",
    "        \"day\": day.strftime(\"%d\"),\n",
    "        \"time\": ERA5_CONFIG[\"times\"],\n",
    "        \"format\": ERA5_CONFIG[\"format\"],\n",
    "    }\n",
    "\n",
    "    print(f\"↓ Download ERA5 humidity data for {day}\")\n",
    "    c.retrieve(\n",
    "        \"reanalysis-era5-pressure-levels\",\n",
    "        request,\n",
    "        str(target_file),\n",
    "    )\n",
    "\n",
    "    # Flag as processed\n",
    "    (day_dir / \"Done.txt\").write_text(\"OK\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0039fe-a7c2-4cab-bad2-623066648a7f",
   "metadata": {},
   "source": [
    "### Central workflow with ERA5 download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "1a738292-f1e0-43c8-bda3-f54b09a2d138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Central control flow that handles the daily ERA5 data\n",
    "def run_daily_era5_workflow(\n",
    "    start_date = None,\n",
    "    end_date = None,\n",
    "    target_date = None, \n",
    "    # ensure proper functionaility when script is executed w/o arguments\n",
    "):\n",
    "\n",
    "    # If the function is called without start_date, find a suitable start date\n",
    "    if start_date is None:\n",
    "        start_date = find_oldest_processed_day(BASE_DIR, \"%Y-%m-%d\")\n",
    "        \n",
    "    # If the function is called without end_date, find a suitable end date    \n",
    "    if end_date is None:\n",
    "        end_date = date.today()\n",
    "        \n",
    "    # Check that the given date range is correct\n",
    "    check_dates(start_date, end_date)\n",
    "        \n",
    "    # Check whether a target date or all days within the range should be processed       \n",
    "    if target_date is not None:\n",
    "        \n",
    "        print(f\"Start workflow for individual day: {target_date}\")\n",
    "        try:\n",
    "            download_era5_humidity(target_date)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {target_date}: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Start workflow for all missing days\")\n",
    "\n",
    "    while True:\n",
    "        missing_day = find_oldest_missing_day(start_date, end_date)\n",
    "        if missing_day is None:\n",
    "            print(\"✓ All days are processed\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            download_era5_humidity(missing_day)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {missing_day}: {e}\")\n",
    "            print(\"→ Error logged, next day will be processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b925bdb5-7faf-4616-a0d0-756863bd7e7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3 - Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6062887e-6216-4bcc-b8a4-70d1a29e0621",
   "metadata": {},
   "source": [
    "### Interpolate daily data batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "024f29dd-bd31-45be-a6ff-8dd366347ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Interpolates daily ERA5 data into a HEALPix grid of nside 8 and 16 using the hpgeom module\n",
    "def interpolate_to_healpix(day, nside_list=[8, 16]):\n",
    "\n",
    "    print(\"Start interpolation...\")\n",
    "    \n",
    "    day_dir = BASE_DIR / day.isoformat()\n",
    "    input_file = day_dir / \"era5_humidity.nc\"\n",
    "\n",
    "    if not input_file.exists():\n",
    "        raise FileNotFoundError(f\"Missing ERA5 file for {day}\")\n",
    "\n",
    "    ds = xr.open_dataset(input_file)\n",
    "\n",
    "    time_dim = \"valid_time\"\n",
    "    level_dim = \"pressure_level\"\n",
    "    q = ds[\"q\"]\n",
    "    lats = ds[\"latitude\"].values\n",
    "    lons = ds[\"longitude\"].values\n",
    "\n",
    "    print(\"...step 1...\")\n",
    "    \n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    src_points = np.column_stack([lat_grid.ravel(), lon_grid.ravel()])\n",
    "\n",
    "    for nside in nside_list:\n",
    "        \n",
    "        print(f\"...nside {nside}...\")\n",
    "        \n",
    "        npix = hpg.nside_to_npixel(nside)\n",
    "        pixels = np.arange(npix)\n",
    "\n",
    "        theta, phi = hpg.pixel_to_angle(nside, pixels)\n",
    "        hp_lats = 90.0 - np.degrees(theta)\n",
    "        hp_lons = np.degrees(phi)\n",
    "        tgt_points = np.column_stack([hp_lats, hp_lons])\n",
    "\n",
    "        hp_q = np.empty(\n",
    "            (q.sizes[time_dim], q.sizes[level_dim], npix)\n",
    "        )\n",
    "\n",
    "        for t in range(q.sizes[time_dim]):\n",
    "            print(f\"...t {t}...\")\n",
    "            for lev in range(q.sizes[level_dim]):\n",
    "                print(f\"...lev {lev}...\")\n",
    "                values = q.isel({time_dim: t, level_dim: lev}).values.ravel()\n",
    "\n",
    "                interp = griddata(\n",
    "                    src_points, values, tgt_points, method=\"linear\"\n",
    "                )\n",
    "\n",
    "                nan_mask = np.isnan(interp)\n",
    "                if np.any(nan_mask):\n",
    "                    interp[nan_mask] = griddata(\n",
    "                        src_points,\n",
    "                        values,\n",
    "                        tgt_points[nan_mask],\n",
    "                        method=\"nearest\"\n",
    "                    )\n",
    "\n",
    "                hp_q[t, lev, :] = interp\n",
    "\n",
    "        out_ds = xr.Dataset(\n",
    "            data_vars={\n",
    "                \"q\": ((time_dim, level_dim, \"pixels\"), hp_q)\n",
    "            },\n",
    "            coords={\n",
    "                time_dim: ds[time_dim],\n",
    "                level_dim: ds[level_dim],\n",
    "                \"pixels\": pixels,\n",
    "            },\n",
    "            attrs={\"healpix_nside\": nside},\n",
    "        )\n",
    "\n",
    "        out_file = day_dir / f\"healpix_n{nside}.nc\"\n",
    "        out_ds.to_netcdf(out_file)\n",
    "\n",
    "        print(f\"✓ {day}: interpolated to HEALPix NSIDE={nside}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9992631-3ecb-4b65-bd39-5a263e41fa45",
   "metadata": {},
   "source": [
    "### Central Workflow with ERA5 download and interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "fe6931ef-5cba-4092-b25e-28cd3a0bf689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_daily_era5_interpolation_workflow(\n",
    "    start_date = None,\n",
    "    end_date = None,\n",
    "    target_date = None, \n",
    "    # ensure proper functionaility when script is executed w/o arguments\n",
    "    nside_list=[8,16],\n",
    "):\n",
    "\n",
    "    # If the function is called without start_date, find a suitable start date\n",
    "    if start_date is None:\n",
    "        start_date = find_oldest_processed_day(BASE_DIR, \"%Y-%m-%d\")\n",
    "    \n",
    "    # If the function is called without end_date, find a suitable end date\n",
    "    if end_date is None:\n",
    "        end_date = date.today()\n",
    "    \n",
    "    # Check that the given date range is correct\n",
    "    check_dates(start_date, end_date)\n",
    "        \n",
    "    # Check whether a target date or all days within the range should be processed\n",
    "    if target_date is not None:\n",
    "        \n",
    "        print(f\"Start workflow for individual day: {target_date}\")\n",
    "        try:\n",
    "            download_era5_humidity(target_date)\n",
    "            interpolate_to_healpix(target_date, nside_list=nside_list)  # HEALPix Interpolation\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {target_date}: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Start workflow for all missing days\")\n",
    "\n",
    "    while True:\n",
    "        missing_day = find_oldest_missing_day(start_date, end_date)\n",
    "        if missing_day is None:\n",
    "            print(\"✓ All days are processed\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            download_era5_humidity(missing_day)\n",
    "            interpolate_to_healpix(missing_day, nside_list=nside_list)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {missing_day}: {e}\")\n",
    "            print(\"→ Error logged, next day will be processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d73a0e-8965-418e-8c27-6751c306206a",
   "metadata": {},
   "source": [
    "## Task 4 - Chunking and save as Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ea5bd-2df6-4448-99bc-8b627bdbd121",
   "metadata": {},
   "source": [
    "### Chunking and save to Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "d48b6044-14dd-42e7-84f4-18385b9d6e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ZARR_DIR = Path(\"processed_data_zarr\")\n",
    "ZARR_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Stores the interpolated HEALPix data for one day in Zarr\n",
    "def save_to_zarr(day, nside):\n",
    "    day_dir = BASE_DIR / day.isoformat()\n",
    "    input_file = day_dir / f\"healpix_n{nside}.nc\"\n",
    "    \n",
    "    if not input_file.exists():\n",
    "        raise FileNotFoundError(f\"No interpolated file for {day} NSIDE={nside} found\")\n",
    "    \n",
    "    ds = xr.open_dataset(input_file)\n",
    "    \n",
    "    # Dynamisches Chunking basierend auf den vorhandenen Dimensionen\n",
    "    # Defining of a chunking strategy, 1 day per chunk\n",
    "    chunk_dict = {}\n",
    "    for dim in ['time', 'level', 'pixels']:\n",
    "        if dim in ds.dims:\n",
    "            chunk_dict[dim] = 1 if dim == 'time' else ds.dims[dim]\n",
    "\n",
    "    ds_chunked = ds.chunk(chunk_dict)\n",
    "    \n",
    "    zarr_dir = Path(\"processed_data_zarr\") / f\"healpix_n{nside}.zarr\"\n",
    "    zarr_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Speichern (Append möglich, falls Zarr schon existiert)\n",
    "    ds_chunked.to_zarr(zarr_dir, mode='a')  # 'a' = append / update\n",
    "    print(f\"✓ Saved day {day} NSIDE={nside} to Zarr: {zarr_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9a64f-19c6-490b-bd62-0e6e8b7dd9b7",
   "metadata": {},
   "source": [
    "### Central workflow with ERA5 download, interpolation, chunking and saving as Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "58e2bd0d-6957-4e43-af37-0615906d3c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "def run_daily_era5_interpolation_workflow_zarr(\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    target_date=None,\n",
    "    # ensure proper functionaility when script is executed w/o arguments\n",
    "    nside_list=[8],\n",
    "):\n",
    "\n",
    "    # If the function is called without start_date, find a suitable start date\n",
    "    if start_date is None:\n",
    "        start_date = find_oldest_processed_day(BASE_DIR, \"%Y-%m-%d\")\n",
    "    \n",
    "    # If the function is called without end_date, find a suitable end date\n",
    "    if end_date is None:\n",
    "        end_date = date.today()\n",
    "        \n",
    "    # Check that the given date range is correct\n",
    "    check_dates(start_date, end_date)\n",
    "    \n",
    "    # Check whether a target date or all days within the range should be processed\n",
    "    if target_date is not None:\n",
    "        print(f\"Start workflow for individual day: {target_date}\")\n",
    "        try:\n",
    "            download_era5_humidity(target_date)\n",
    "            interpolate_to_healpix(target_date, nside_list=nside_list)\n",
    "            \n",
    "            for nside in nside_list:\n",
    "                save_to_zarr(target_date, nside)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {target_date}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Workflow when a date range is downloaded\n",
    "    print(\"Start workflow for all missing days\")\n",
    "\n",
    "    while True:\n",
    "        missing_day = find_oldest_missing_day(start_date, end_date)\n",
    "        if missing_day is None:\n",
    "            print(\"✓ All days are processed\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            download_era5_humidity(missing_day)\n",
    "            interpolate_to_healpix(missing_day, nside_list=nside_list)\n",
    "\n",
    "            for nside in nside_list:\n",
    "                save_to_zarr(missing_day, nside)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {missing_day}: {e}\")\n",
    "            print(\"→ Error logged, next day will be processed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468d192-c45f-46cb-b235-735446f49106",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "135177fc-33b0-42a5-bd37-20331835b3a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Das System kann den angegebenen Pfad nicht finden: 'processed_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[279], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Test ERA5 download routine with interpolation\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m run_daily_era5_interpolation_workflow(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m#start_date=date(2024, 12, 1),\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#end_date=date(2024, 12, 1),\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     target_date\u001b[38;5;241m=\u001b[39mdate(\u001b[38;5;241m2024\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      7\u001b[0m )\n",
      "Cell \u001b[1;32mIn[258], line 11\u001b[0m, in \u001b[0;36mrun_daily_era5_interpolation_workflow\u001b[1;34m(start_date, end_date, target_date, nside_list)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_daily_era5_interpolation_workflow\u001b[39m(\n\u001b[0;32m      2\u001b[0m     start_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m      3\u001b[0m     end_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# If the function is called without start_date, find a suitable start date\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m         start_date \u001b[38;5;241m=\u001b[39m find_oldest_processed_day(BASE_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# If the function is called without end_date, find a suitable end date\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end_date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m, in \u001b[0;36mfind_oldest_processed_day\u001b[1;34m(processed_dir, date_format)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_oldest_processed_day\u001b[39m(processed_dir, date_format):\n\u001b[0;32m      4\u001b[0m     oldest_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(processed_dir):\n\u001b[0;32m      7\u001b[0m         folder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(processed_dir, entry)\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(folder_path):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Das System kann den angegebenen Pfad nicht finden: 'processed_data'"
     ]
    }
   ],
   "source": [
    "# Test ERA5 download routine with interpolation\n",
    "\n",
    "run_daily_era5_interpolation_workflow(\n",
    "    #start_date=date(2024, 12, 1),\n",
    "    #end_date=date(2024, 12, 1),\n",
    "    target_date=date(2024, 12, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "dc9551e1-8eee-458c-ad5b-4a03fc338805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start workflow for individual day: 2024-12-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 09:13:30,075 INFO Request ID is 1712da6d-ebdd-4d37-9e6f-780077b608aa\n",
      "2026-01-20 09:13:30,124 INFO status has been updated to accepted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↓ Download ERA5 humidity data for 2024-12-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 09:14:19,893 INFO status has been updated to running\n",
      "2026-01-20 09:14:45,578 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "791846caa92b6f74f6d77e87ce88151f.nc:   0%|          | 0.00/33.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start interpolation...\n",
      "...step 1...\n",
      "...nside 8...\n",
      "...t 0...\n",
      "...lev 0...\n",
      "...lev 1...\n",
      "...lev 2...\n",
      "...lev 3...\n",
      "...lev 4...\n",
      "...t 1...\n",
      "...lev 0...\n",
      "...lev 1...\n",
      "...lev 2...\n",
      "...lev 3...\n",
      "...lev 4...\n",
      "...t 2...\n",
      "...lev 0...\n",
      "...lev 1...\n",
      "...lev 2...\n",
      "...lev 3...\n",
      "...lev 4...\n",
      "...t 3...\n",
      "...lev 0...\n",
      "...lev 1...\n",
      "...lev 2...\n",
      "...lev 3...\n",
      "...lev 4...\n",
      "✓ 2024-12-02: interpolated to HEALPix NSIDE=8\n",
      "✗ Error on 2024-12-02: Expected an integer or an iterable of integers. Got None instead.\n"
     ]
    }
   ],
   "source": [
    "run_daily_era5_interpolation_workflow_zarr(\n",
    "    target_date=date(2024, 12, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "cc438524-9fac-4f4a-b4b9-fb24db0d4076",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start workflow for individual day: 2024-12-03\n",
      "↓ Download ERA5 humidity data for 2024-12-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 10:12:03,289 INFO Request ID is 65979b48-6cf7-4d43-ab48-b8e6730562ee\n",
      "2026-01-20 10:12:03,361 INFO status has been updated to accepted\n",
      "2026-01-20 10:12:24,517 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "e74b3502a99cbf498d1589e6e42a1f52.nc:   0%|          | 0.00/33.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start interpolation...\n",
      "...step 1...\n",
      "...nside 8...\n",
      "...t 0...\n",
      "...lev 0...\n",
      "...lev 1...\n",
      "...lev 2...\n",
      "...lev 3...\n",
      "...lev 4...\n",
      "...t 1...\n",
      "...lev 0...\n",
      "...lev 1...\n",
      "...lev 2...\n",
      "...lev 3...\n",
      "...lev 4...\n",
      "...t 2...\n",
      "...lev 0...\n",
      "...lev 1...\n",
      "...lev 2...\n",
      "...lev 3...\n",
      "...lev 4...\n",
      "...t 3...\n",
      "...lev 0...\n",
      "...lev 1...\n",
      "...lev 2...\n",
      "...lev 3...\n",
      "...lev 4...\n",
      "✓ 2024-12-03: interpolated to HEALPix NSIDE=8\n",
      "✗ Error on 2024-12-03: Expected an integer or an iterable of integers. Got None instead.\n"
     ]
    }
   ],
   "source": [
    "run_daily_era5_interpolation_workflow_zarr(\n",
    "    start_date=date(2024, 12, 1),\n",
    "    end_date=date(2024, 12, 5),\n",
    "    target_date=date(2024, 12, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfe667-ac4e-44e9-91b2-0e7f6ea482d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
