{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef487390-5a23-4274-8c8f-ea421dbf6d65",
   "metadata": {},
   "source": [
    "# Homework Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "879322dd-acba-447d-aedb-86ee691ea8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3254bb69-5090-4893-9710-dd6d99266e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numcodecs 0.16.3\n",
      "Uninstalling numcodecs-0.16.3:\n",
      "  Successfully uninstalled numcodecs-0.16.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Nils\\AppData\\Local\\Temp\\pip-uninstall-dru76qri'.\n",
      "You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numcodecs==0.11.0\n",
      "  Downloading numcodecs-0.11.0-cp311-cp311-win_amd64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\nils\\anaconda3\\lib\\site-packages (from numcodecs==0.11.0) (0.4)\n",
      "Requirement already satisfied: numpy>=1.7 in c:\\users\\nils\\appdata\\roaming\\python\\python311\\site-packages (from numcodecs==0.11.0) (1.26.4)\n",
      "Downloading numcodecs-0.11.0-cp311-cp311-win_amd64.whl (599 kB)\n",
      "   ---------------------------------------- 0.0/599.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 599.9/599.9 kB 7.2 MB/s  0:00:00\n",
      "Installing collected packages: numcodecs\n",
      "Successfully installed numcodecs-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numcodecs\n",
    "!pip install \"numcodecs==0.11.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f766aeba-52a8-46cc-a336-52c41bba5f11",
   "metadata": {},
   "source": [
    "### Check necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c74313b3-8906-497f-8fc6-ac433ead763d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import zarr\n",
    "except ImportError:\n",
    "    !pip install zarr\n",
    "    import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0c1aff0-2eae-4c13-9275-d28cb54eb70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import hpgeom as hpg\n",
    "except ImportError:\n",
    "    !pip install hpgeom\n",
    "    import hpgeom as hpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc865c16-4646-4504-b022-c9ee7104886c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import cdsapi\n",
    "except ImportError:\n",
    "    !pip install cdsapi\n",
    "    import cdsapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e90052-8c51-4fe8-acad-faa33836f8b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 1 - Core control flow & mock-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3f17e-1c5d-4c93-b986-4d3718d1c860",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Date Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ce8c06-dde3-47f3-b2ef-944028e74b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generator function to find and store all dates inside of the given range, inclusive the start and end date \n",
    "def daterange(start, end):\n",
    "    \n",
    "    current = start\n",
    "    while current <= end:\n",
    "        yield current\n",
    "        current += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57562249-f1b5-495f-b20a-bbb1dabcb48f",
   "metadata": {},
   "source": [
    "### Status Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5109d777-de6c-4dc6-a39c-c72412f821ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If necessary, a new directory will be created\n",
    "BASE_DIR = Path(\"processed_data\")\n",
    "BASE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Checks whether a date has already been processed\n",
    "def is_day_processed(day: date) -> bool:\n",
    "    # A date is considered processed if a folder exists for it\n",
    "    return (BASE_DIR / day.isoformat()).exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a068e2ef-53d7-4589-b984-e469b5d42b6d",
   "metadata": {},
   "source": [
    "### Mock-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d690175-bfa4-4acb-929a-5780de72007c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mock-processing that simulates download, processing and archiving of daily files\n",
    "def mock_process_day(day, fail_probability=0.2):\n",
    "    print(f\"→ Processing {day}\")\n",
    "\n",
    "    # Simulating a random error during download\n",
    "    if random.random() < fail_probability:\n",
    "        raise RuntimeError(\"Mock download failed\")\n",
    "\n",
    "    # Save the daily mock file\n",
    "    day_dir = BASE_DIR / day.isoformat()\n",
    "    day_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with open(day_dir / \"done.txt\", \"w\") as f:\n",
    "        f.write(f\"Processed on {datetime.now()}\\n\")\n",
    "\n",
    "    print(f\"✓ {day} successfully processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2559f1bf-6513-4503-994d-49427991b130",
   "metadata": {},
   "source": [
    "### Find oldest missing day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13fc0a0-b044-48e5-8d84-ddc18bae9457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the oldest missing day of a given date range \n",
    "def find_oldest_missing_day(start, end) -> date | None:\n",
    "    for d in daterange(start, end):\n",
    "        # Use the established logic\n",
    "        if not is_day_processed(d):\n",
    "            return d\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1201cef-24e3-429e-9c11-a5222195ff03",
   "metadata": {},
   "source": [
    "### Find oldest processed day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c68463b-d346-4e90-88c4-457c1267388e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the oldest fully processed day in the directory\n",
    "def find_oldest_processed_day(processed_dir, date_format):\n",
    "    \n",
    "    oldest_date = None\n",
    "    \n",
    "    for entry in os.listdir(processed_dir):\n",
    "        folder_path = os.path.join(processed_dir, entry)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue  # Skip files, just looking for folders\n",
    "        \n",
    "        try:\n",
    "            folder_date = datetime.strptime(entry, date_format).date() # Extract date from folder name\n",
    "        except ValueError:\n",
    "            # Ignore files that do not match the given date format\n",
    "            continue\n",
    "\n",
    "        # Check whether “Done.txt” exists in the folder\n",
    "        done_file = os.path.join(folder_path, \"Done.txt\")\n",
    "        if not os.path.isfile(done_file):\n",
    "            continue  # Date has not been fully processed\n",
    "            \n",
    "        if oldest_date is None or folder_date < oldest_date:\n",
    "            oldest_date = folder_date\n",
    "\n",
    "    return oldest_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b296a33-96a1-4992-b6b9-afd02150b7e0",
   "metadata": {},
   "source": [
    "### Check dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "432e3b97-81ff-4b8c-ba3e-baa681fcd69d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_dates(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    target_date = None\n",
    "):\n",
    "    if end_date and start_date is not None:\n",
    "        if end_date < start_date:\n",
    "            raise ValueError(\"✗ End_date is before start_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339679a-458d-4343-b515-05aa63ad45cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Central Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ed5a70f-2f52-40fe-be18-8322917f9f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Central control flow that handles the daily data\n",
    "def run_daily_workflow(\n",
    "    start_date = None,\n",
    "    end_date = None,\n",
    "    target_date = None, \n",
    "    # ensure proper functionaility when script is executed w/o arguments\n",
    "):\n",
    "\n",
    "    # If the function is called without start_date, find a suitable start date\n",
    "    if start_date is None:\n",
    "        start_date = find_oldest_processed_day(BASE_DIR, \"%Y-%m-%d\")\n",
    "    \n",
    "    # If the function is called without end_date, find a suitable end date\n",
    "    if end_date is None:\n",
    "        end_date = date.today()\n",
    "    \n",
    "    # Check that the given date range is correct\n",
    "    check_dates(start_date, end_date)\n",
    "    \n",
    "    # Check whether a target date or all days within the range should be processed\n",
    "    if target_date is not None:\n",
    "            \n",
    "        print(f\"Start workflow for individual target date: {target_date}\")\n",
    "        try:\n",
    "            mock_process_day(target_date)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {target_date}: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Start workflow for all missing days\")\n",
    "\n",
    "    while True:\n",
    "        missing_day = find_oldest_missing_day(start_date, end_date)\n",
    "        if missing_day is None:\n",
    "            print(\"✓ All days are processed\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            mock_process_day(missing_day)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {missing_day}: {e}\")\n",
    "            print(\"→ Error logged, next day will be processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c20ef-92c3-47b3-b414-122925f76081",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2 - Flexible ERA5 donwload routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3381c004-038c-47fd-86e7-765048e7789d",
   "metadata": {},
   "source": [
    "### ERA5 configuration dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7363308-f7e6-46e5-b1eb-9702606e3c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This dictionary is used to easily adapt the donwload routine to other parameter values as stated in the assignment.\n",
    "# No hardcoded settings within the download routine of the core workflow  \n",
    "\n",
    "ERA5_CONFIG = {\n",
    "    \"dataset\": \"reanalysis-era5-pressure-levels\",\n",
    "    \"format\": \"netcdf\",\n",
    "    \"variable\": [\"specific_humidity\"],\n",
    "    \"pressure_levels\": [975, 900, 800, 500, 300],\n",
    "    \"times\": [\"00:00\", \"06:00\", \"12:00\", \"18:00\"],\n",
    "    \"filename\": \"era5_humidity.nc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2779b3-97f3-4b98-bd19-80d30d816436",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download ERA5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d04932d0-cfc6-497a-b187-c4aea7699eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downloads ERA5 humidity data for a single given day\n",
    "def download_era5_humidity(day):\n",
    "\n",
    "    # Save the daily data\n",
    "    day_dir = BASE_DIR / day.isoformat()\n",
    "    day_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    target_file = day_dir / \"era5_humidity.nc\"\n",
    "\n",
    "    # CDS API request\n",
    "    c = cdsapi.Client()\n",
    "    request = {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": ERA5_CONFIG[\"variable\"],\n",
    "        \"pressure_level\": [str(p) for p in ERA5_CONFIG[\"pressure_levels\"]],\n",
    "        \"year\": day.strftime(\"%Y\"),\n",
    "        \"month\": day.strftime(\"%m\"),\n",
    "        \"day\": day.strftime(\"%d\"),\n",
    "        \"time\": ERA5_CONFIG[\"times\"],\n",
    "        \"format\": ERA5_CONFIG[\"format\"],\n",
    "    }\n",
    "\n",
    "    print(f\"↓ Download ERA5 humidity data for {day}\")\n",
    "    c.retrieve(\n",
    "        \"reanalysis-era5-pressure-levels\",\n",
    "        request,\n",
    "        str(target_file),\n",
    "    )\n",
    "\n",
    "    # Flag as processed\n",
    "    (day_dir / \"Done.txt\").write_text(\"OK\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0039fe-a7c2-4cab-bad2-623066648a7f",
   "metadata": {},
   "source": [
    "### Central workflow with ERA5 download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a738292-f1e0-43c8-bda3-f54b09a2d138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Central control flow that handles the daily ERA5 data\n",
    "def run_daily_era5_workflow(\n",
    "    start_date = None,\n",
    "    end_date = None,\n",
    "    target_date = None, \n",
    "    # ensure proper functionaility when script is executed w/o arguments\n",
    "):\n",
    "\n",
    "    # If the function is called without start_date, find a suitable start date\n",
    "    if start_date is None:\n",
    "        start_date = find_oldest_processed_day(BASE_DIR, \"%Y-%m-%d\")\n",
    "        \n",
    "    # If the function is called without end_date, find a suitable end date    \n",
    "    if end_date is None:\n",
    "        end_date = date.today()\n",
    "        \n",
    "    # Check that the given date range is correct\n",
    "    check_dates(start_date, end_date)\n",
    "        \n",
    "    # Check whether a target date or all days within the range should be processed       \n",
    "    if target_date is not None:\n",
    "        \n",
    "        print(f\"Start workflow for individual day: {target_date}\")\n",
    "        try:\n",
    "            download_era5_humidity(target_date)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {target_date}: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Start workflow for all missing days\")\n",
    "\n",
    "    while True:\n",
    "        missing_day = find_oldest_missing_day(start_date, end_date)\n",
    "        if missing_day is None:\n",
    "            print(\"✓ All days are processed\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            download_era5_humidity(missing_day)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {missing_day}: {e}\")\n",
    "            print(\"→ Error logged, next day will be processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b925bdb5-7faf-4616-a0d0-756863bd7e7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3 - Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6062887e-6216-4bcc-b8a4-70d1a29e0621",
   "metadata": {},
   "source": [
    "### Interpolate daily data batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "024f29dd-bd31-45be-a6ff-8dd366347ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Interpolates daily ERA5 data into a HEALPix grid of nside 8 and 16 using the hpgeom module\n",
    "def interpolate_to_healpix(day, nside_list=[8, 16]):\n",
    "\n",
    "    print(\"Start interpolation...\")\n",
    "    \n",
    "    day_dir = BASE_DIR / day.isoformat()\n",
    "    input_file = day_dir / \"era5_humidity.nc\"\n",
    "\n",
    "    if not input_file.exists():\n",
    "        raise FileNotFoundError(f\"Missing ERA5 file for {day}\")\n",
    "\n",
    "    ds = xr.open_dataset(input_file)\n",
    "\n",
    "    time_dim = \"valid_time\"\n",
    "    level_dim = \"pressure_level\"\n",
    "    q = ds[\"q\"]\n",
    "    lats = ds[\"latitude\"].values\n",
    "    lons = ds[\"longitude\"].values\n",
    "\n",
    "    print(\"...step 1...\")\n",
    "    \n",
    "    lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "    src_points = np.column_stack([lat_grid.ravel(), lon_grid.ravel()])\n",
    "\n",
    "    for nside in nside_list:\n",
    "        \n",
    "        print(f\"...nside {nside}...\")\n",
    "        \n",
    "        npix = hpg.nside_to_npixel(nside)\n",
    "        pixels = np.arange(npix)\n",
    "\n",
    "        theta, phi = hpg.pixel_to_angle(nside, pixels)\n",
    "        hp_lats = 90.0 - np.degrees(theta)\n",
    "        hp_lons = np.degrees(phi)\n",
    "        tgt_points = np.column_stack([hp_lats, hp_lons])\n",
    "\n",
    "        hp_q = np.empty(\n",
    "            (q.sizes[time_dim], q.sizes[level_dim], npix)\n",
    "        )\n",
    "\n",
    "        for t in range(q.sizes[time_dim]):\n",
    "            print(f\"...t {t}...\")\n",
    "            for lev in range(q.sizes[level_dim]):\n",
    "                print(f\"...lev {lev}...\")\n",
    "                values = q.isel({time_dim: t, level_dim: lev}).values.ravel()\n",
    "\n",
    "                interp = griddata(\n",
    "                    src_points, values, tgt_points, method=\"linear\"\n",
    "                )\n",
    "\n",
    "                nan_mask = np.isnan(interp)\n",
    "                if np.any(nan_mask):\n",
    "                    interp[nan_mask] = griddata(\n",
    "                        src_points,\n",
    "                        values,\n",
    "                        tgt_points[nan_mask],\n",
    "                        method=\"nearest\"\n",
    "                    )\n",
    "\n",
    "                hp_q[t, lev, :] = interp\n",
    "\n",
    "        out_ds = xr.Dataset(\n",
    "            data_vars={\n",
    "                \"q\": ((time_dim, level_dim, \"pixels\"), hp_q)\n",
    "            },\n",
    "            coords={\n",
    "                time_dim: ds[time_dim],\n",
    "                level_dim: ds[level_dim],\n",
    "                \"pixels\": pixels,\n",
    "            },\n",
    "            attrs={\"healpix_nside\": nside},\n",
    "        )\n",
    "\n",
    "        out_file = day_dir / f\"healpix_n{nside}.nc\"\n",
    "        out_ds.to_netcdf(out_file)\n",
    "\n",
    "        print(f\"✓ {day}: interpolated to HEALPix NSIDE={nside}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9992631-3ecb-4b65-bd39-5a263e41fa45",
   "metadata": {},
   "source": [
    "### Central Workflow with ERA5 download and interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe6931ef-5cba-4092-b25e-28cd3a0bf689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_daily_era5_interpolation_workflow(\n",
    "    start_date = None,\n",
    "    end_date = None,\n",
    "    target_date = None, \n",
    "    # ensure proper functionaility when script is executed w/o arguments\n",
    "    nside_list=[8,16],\n",
    "):\n",
    "\n",
    "    # If the function is called without start_date, find a suitable start date\n",
    "    if start_date is None:\n",
    "        start_date = find_oldest_processed_day(BASE_DIR, \"%Y-%m-%d\")\n",
    "    \n",
    "    # If the function is called without end_date, find a suitable end date\n",
    "    if end_date is None:\n",
    "        end_date = date.today()\n",
    "    \n",
    "    # Check that the given date range is correct\n",
    "    check_dates(start_date, end_date)\n",
    "        \n",
    "    # Check whether a target date or all days within the range should be processed\n",
    "    if target_date is not None:\n",
    "        \n",
    "        print(f\"Start workflow for individual day: {target_date}\")\n",
    "        try:\n",
    "            download_era5_humidity(target_date)\n",
    "            interpolate_to_healpix(target_date, nside_list=nside_list)  # HEALPix Interpolation\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {target_date}: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Start workflow for all missing days\")\n",
    "\n",
    "    while True:\n",
    "        missing_day = find_oldest_missing_day(start_date, end_date)\n",
    "        if missing_day is None:\n",
    "            print(\"✓ All days are processed\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            download_era5_humidity(missing_day)\n",
    "            interpolate_to_healpix(missing_day, nside_list=nside_list)\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {missing_day}: {e}\")\n",
    "            print(\"→ Error logged, next day will be processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d73a0e-8965-418e-8c27-6751c306206a",
   "metadata": {},
   "source": [
    "## Task 4 - Chunking and save as Zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ea5bd-2df6-4448-99bc-8b627bdbd121",
   "metadata": {},
   "source": [
    "### Chunking and save to Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d48b6044-14dd-42e7-84f4-18385b9d6e23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ZARR_DIR = Path(\"processed_data_zarr\")\n",
    "ZARR_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def save_to_zarr(day, nside, base_dir=BASE_DIR, zarr_root=Path(\"processed_data_zarr\")):\n",
    "    day_dir = base_dir / day.isoformat()\n",
    "    input_file = day_dir / f\"healpix_n{nside}.nc\"\n",
    "    \n",
    "    if not input_file.exists():\n",
    "        raise FileNotFoundError(f\"No interpolated file for {day} NSIDE={nside} found\")\n",
    "    \n",
    "    with xr.open_dataset(input_file) as ds:\n",
    "\n",
    "        chunk_dict = {dim: 1 if dim == \"valid_time\" else ds.dims[dim] for dim in ds.dims}\n",
    "        ds_chunked = ds.chunk(chunk_dict)\n",
    "        \n",
    "        zarr_dir = zarr_root / f\"healpix_n{nside}.zarr\"\n",
    "        \n",
    "        if not zarr_dir.exists():\n",
    "            ds_chunked.to_zarr(zarr_dir, mode=\"w\")\n",
    "            print(f\"✓ Initial Zarr store created: {zarr_dir} (day {day})\")\n",
    "            return \n",
    "        \n",
    "        ds_chunked.to_zarr(zarr_dir, mode=\"a\", append_dim=\"valid_time\")\n",
    "        print(f\"✓ Appended day {day} NSIDE={nside} to Zarr: {zarr_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf9a64f-19c6-490b-bd62-0e6e8b7dd9b7",
   "metadata": {},
   "source": [
    "### Central workflow with ERA5 download, interpolation, chunking and saving as Zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58e2bd0d-6957-4e43-af37-0615906d3c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "def run_daily_era5_interpolation_workflow_zarr(\n",
    "    start_date=None,\n",
    "    end_date=None,\n",
    "    target_date=None,\n",
    "    # ensure proper functionaility when script is executed w/o arguments\n",
    "    nside_list=[8,16],\n",
    "):\n",
    "\n",
    "    # If the function is called without start_date, find a suitable start date\n",
    "    if start_date is None:\n",
    "        start_date = find_oldest_processed_day(BASE_DIR, \"%Y-%m-%d\")\n",
    "    \n",
    "    # If the function is called without end_date, find a suitable end date\n",
    "    if end_date is None:\n",
    "        end_date = date.today()\n",
    "        \n",
    "    # Check that the given date range is correct\n",
    "    check_dates(start_date, end_date)\n",
    "    \n",
    "    # Check whether a target date or all days within the range should be processed\n",
    "    if target_date is not None:\n",
    "        print(f\"Start workflow for individual day: {target_date}\")\n",
    "        try:\n",
    "            #download_era5_humidity(target_date)\n",
    "            #interpolate_to_healpix(target_date, nside_list=nside_list)\n",
    "            \n",
    "            for nside in nside_list:\n",
    "                save_to_zarr(target_date, nside)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {target_date}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Workflow when a date range is downloaded\n",
    "    print(\"Start workflow for all missing days\")\n",
    "\n",
    "    while True:\n",
    "        missing_day = find_oldest_missing_day(start_date, end_date)\n",
    "        if missing_day is None:\n",
    "            print(\"✓ All days are processed\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            download_era5_humidity(missing_day)\n",
    "            interpolate_to_healpix(missing_day, nside_list=nside_list)\n",
    "\n",
    "            for nside in nside_list:\n",
    "                save_to_zarr(missing_day, nside)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error on {missing_day}: {e}\")\n",
    "            print(\"→ Error logged, next day will be processed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468d192-c45f-46cb-b235-735446f49106",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24519358-2ee5-420c-a2a2-8c440adf78e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:         (valid_time: 4, pressure_level: 5, pixels: 768)\n",
      "Coordinates:\n",
      "  * valid_time      (valid_time) datetime64[ns] 2024-12-02 ... 2024-12-02T18:...\n",
      "  * pressure_level  (pressure_level) float64 975.0 900.0 800.0 500.0 300.0\n",
      "  * pixels          (pixels) int32 0 1 2 3 4 5 6 ... 761 762 763 764 765 766 767\n",
      "Data variables:\n",
      "    q               (valid_time, pressure_level, pixels) float64 ...\n",
      "Attributes:\n",
      "    healpix_nside:  8\n",
      "\n",
      "Dimensions: Frozen({'valid_time': 4, 'pressure_level': 5, 'pixels': 768})\n",
      "\n",
      "Coordinates: Coordinates:\n",
      "  * valid_time      (valid_time) datetime64[ns] 2024-12-02 ... 2024-12-02T18:...\n",
      "  * pressure_level  (pressure_level) float64 975.0 900.0 800.0 500.0 300.0\n",
      "  * pixels          (pixels) int32 0 1 2 3 4 5 6 ... 761 762 763 764 765 766 767\n"
     ]
    }
   ],
   "source": [
    "file_path = Path(r\"C:\\Users\\Nils\\earth-system-data-processing\\data_access\\homework02\\processed_data\\2024-12-02\\healpix_n8.nc\")\n",
    "\n",
    "ds = xr.open_dataset(file_path)\n",
    "print(ds)\n",
    "print(\"\\nDimensions:\", ds.dims)\n",
    "print(\"\\nCoordinates:\", ds.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc9551e1-8eee-458c-ad5b-4a03fc338805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start workflow for individual day: 2024-12-02\n",
      "✓ Initial Zarr store created: processed_data_zarr\\healpix_n8.zarr (day 2024-12-02)\n",
      "✓ Initial Zarr store created: processed_data_zarr\\healpix_n16.zarr (day 2024-12-02)\n"
     ]
    }
   ],
   "source": [
    "run_daily_era5_interpolation_workflow_zarr(\n",
    "    target_date=date(2024, 12, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "409c084c-8252-4452-b3ff-a12ae1cf0b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:         (pixels: 768, pressure_level: 5, valid_time: 4)\n",
      "Coordinates:\n",
      "  * pixels          (pixels) int32 0 1 2 3 4 5 6 ... 761 762 763 764 765 766 767\n",
      "  * pressure_level  (pressure_level) float64 975.0 900.0 800.0 500.0 300.0\n",
      "  * valid_time      (valid_time) datetime64[ns] 2024-12-02 ... 2024-12-02T18:...\n",
      "Data variables:\n",
      "    q               (valid_time, pressure_level, pixels) float64 dask.array<chunksize=(1, 5, 768), meta=np.ndarray>\n",
      "Attributes:\n",
      "    healpix_nside:  8\n",
      "Chunks: ('q', ((1, 1, 1, 1), (5,), (768,)))\n"
     ]
    }
   ],
   "source": [
    "ds = xr.open_zarr(r\"C:\\Users\\Nils\\earth-system-data-processing\\data_access\\homework02\\processed_data_zarr\\healpix_n8.zarr\")\n",
    "print(ds)\n",
    "for var in ds.data_vars:\n",
    "    print(f\"Chunks: {var, ds[var].chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9670b1e-c150-4232-83fe-fb391423427f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
