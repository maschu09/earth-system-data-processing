{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import cdsapi\n",
    "import healpy as hp\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import zarr\n",
    "\n",
    "#additional packages that are required to run this project are h5netcdf, netcdf4, scipy, zarr"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CONFIG\n",
    "\n",
    "#====== download config ======#\n",
    "collection='reanalysis-era5-pressure-levels' #this notebook was only tested for the specified collection\n",
    "\n",
    "variables=['specific_humidity'] # other variables can be found at https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation#ERA5:datadocumentation-Table9. This notebook was only developed for continues pressure level variables\n",
    "\n",
    "pressure_levels = ['300', '500', '800', '900', '975']\n",
    "\n",
    "temporal_extend = {\n",
    "    \"start\": '2024-12-01',\n",
    "    \"end\": '2024-12-05',\n",
    "    \"times\": ['00:00:00', '06:00:00', '12:00:00', '18:00:00'],\n",
    "}\n",
    "\n",
    "spacial_extend = 'global' #for a limited area provide an array of shape [north, west, south, east]. north, south of range [-90, 90] west, east [-180, 180]\n",
    "\n",
    "data_format = 'netcdf' # Downstream Processing requires netcdf so only change this variable if you want to change the processing pipeline\n",
    "\n",
    "grid = ['0.25', '0.25'] # [res_long, res_lat] resolution of the grid for the download. The specified 0.25Â° are the default resolution for atmospheric data in the ERA5 Dataset\n",
    "\n",
    "#====== storage config =======#\n",
    "collection_short_name='era5' #short name used for data_storage\n",
    "storage_path='../data/'\n",
    "unprocessed_path=f'{collection_short_name}_regular_grid/'\n",
    "zarr_file=f'{collection_short_name}_healpix.zarr'\n",
    "\n",
    "#===== regridding config =====#\n",
    "\n",
    "nsides = [8, 16]\n",
    "interpolation_method = 'linear' # methode for the interpolation to healpix (one of \"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"quintic\", \"polynomial\")"
   ],
   "id": "ea1bf57cb8e05ee8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not Path(storage_path).exists():\n",
    "    Path(storage_path).mkdir()\n",
    "\n",
    "if not Path(storage_path + unprocessed_path).exists():\n",
    "    Path(storage_path + unprocessed_path).mkdir()"
   ],
   "id": "5178bd5f7aa85fd5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# This cell iterates over the time period defined in temporal_extend and creates a dict containing day month and year for each day in the interval. This will later be used for the main downloading loop\n",
    "start_date = datetime.strptime(temporal_extend[\"start\"], '%Y-%m-%d')\n",
    "end_date = datetime.strptime(temporal_extend[\"end\"], '%Y-%m-%d')\n",
    "\n",
    "time_chunks = []\n",
    "for delta in range((end_date - start_date).days + 1): # iterate over the date range to create a dict for each day in our time_chunks list. Later we can loop over those list entries to loop over the days\n",
    "    i_date = start_date + timedelta(days=delta)\n",
    "    time_chunks.append({\n",
    "        \"day\": i_date.day,\n",
    "        \"month\": i_date.month,\n",
    "        \"year\": i_date.year,\n",
    "        \"string\": i_date.strftime(\"%Y-%m-%d\")\n",
    "    })\n",
    "\n",
    "time_chunks[:3] # :3 needed to limit console size for bigger temporal extends"
   ],
   "id": "a8b46348c7b7f540",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# extract the calculations for the healpix grids out of the main routine, so we don't do them multiple times (potentially for a lot of days / nside resolutions, amounting to a lot of calculations)\n",
    "hp_grids = []\n",
    "\n",
    "for nside in nsides: # iterate over nside resolutions and create the grid so we don't have to do it multiple tim\n",
    "    n_pix = hp.nside2npix(nside) # calculate number of pixels\n",
    "    theta, phi = hp.pix2ang(nside, np.arange(n_pix)) # calculate the pixel centers for each pixel in rad\n",
    "\n",
    "    hp_latitude = 90 - np.degrees(theta) # convert to degrees (North Pole = 0, South Pole = 180) and subtract from 90 to get to (North Pole = 90, South Pole = -90)\n",
    "    hp_longitude = np.degrees(phi) # convert to degrees (Greenwich = 0, increasing eastwards)\n",
    "\n",
    "    hp_grids.append({\"latitude\": hp_latitude, \"longitude\": hp_longitude, \"name\": f'nside-{nside}'})"
   ],
   "id": "6b09c553da7ed0d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "c = cdsapi.Client()\n",
    "\n",
    "zarr_path = storage_path + zarr_file\n",
    "\n",
    "for time_chunk in time_chunks:\n",
    "    file_name = f'{collection_short_name}_{time_chunk[\"string\"]}'\n",
    "    complete_path = storage_path + unprocessed_path + file_name\n",
    "\n",
    "    c.retrieve(collection, {\n",
    "        \"product_type\": ['reanalysis'],\n",
    "        \"variable\": variables,\n",
    "        \"year\": time_chunk[\"year\"],\n",
    "        \"month\": time_chunk[\"month\"],\n",
    "        \"day\": time_chunk[\"day\"],\n",
    "        \"time\": temporal_extend[\"times\"],\n",
    "        \"pressure_level\": pressure_levels,\n",
    "        \"grid\": grid,\n",
    "        \"area\": spacial_extend,\n",
    "       \"data_format\": data_format,\n",
    "    }, complete_path)\n",
    "\n",
    "    ds = xr.open_dataset(complete_path)\n",
    "    ds = ds.sortby('latitude')\n",
    "\n",
    "    for hp_grid in hp_grids:\n",
    "        ds_hp = ds.interp(\n",
    "            latitude=xr.DataArray(hp_grid[\"latitude\"]),\n",
    "            longitude=xr.DataArray(hp_grid[\"longitude\"]),\n",
    "            method=interpolation_method\n",
    "        )\n",
    "\n",
    "        group_path = hp_grid[\"name\"]\n",
    "\n",
    "        if not Path(zarr_path).exists() or group_path not in zarr.open_group(zarr_path, mode=\"a\").group_keys() or \"time\" not in xr.open_zarr(zarr_path, group=group_path).dims:\n",
    "            ds.to_zarr(zarr_path, group=group_path, mode=\"w\")\n",
    "        else:\n",
    "            ds.to_zarr(zarr_path, group=group_path, mode=\"a\", append_dim=\"time\")"
   ],
   "id": "2eb425472bbda376",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
