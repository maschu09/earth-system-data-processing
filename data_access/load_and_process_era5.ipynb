{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-21T13:29:41.077171Z",
     "start_time": "2026-01-21T13:29:41.073840Z"
    }
   },
   "source": [
    "import cdsapi\n",
    "import healpy as hp\n",
    "from datetime import datetime, timedelta\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import zarr\n",
    "from IPython.core.formatters import catch_format_error\n",
    "\n",
    "#additional packages that are required to run this project are h5netcdf, netcdf4, scipy, zarr"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T13:29:41.089391Z",
     "start_time": "2026-01-21T13:29:41.085792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CONFIG\n",
    "\n",
    "#====== download config ======#\n",
    "collection='reanalysis-era5-pressure-levels' #this notebook was only tested for the specified collection\n",
    "\n",
    "variables=['specific_humidity'] # other variables can be found at https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation#ERA5:datadocumentation-Table9. This notebook was only developed for continues pressure level variables\n",
    "\n",
    "pressure_levels = [300, 500, 800, 900, 975]\n",
    "\n",
    "temporal_extend = {\n",
    "    \"start\": '2024-12-01',\n",
    "    \"end\": '2024-12-03',\n",
    "    \"times\": ['00:00:00', '06:00:00', '12:00:00', '18:00:00'],\n",
    "}\n",
    "\n",
    "spacial_extend = 'global' #for a limited area provide an array of shape [north, west, south, east]. north, south of range [-90, 90] west, east [-180, 180]\n",
    "\n",
    "data_format = {\"format\": 'netcdf', \"file_ending\": '.nc'} # Downstream Processing requires netcdf so only change this variable if you want to change the processing pipeline\n",
    "\n",
    "grid = ['0.25', '0.25'] # [res_long, res_lat] resolution of the grid for the download. The specified 0.25Â° are the default resolution for atmospheric data in the ERA5 Dataset\n",
    "\n",
    "#====== storage config =======#\n",
    "collection_short_name='era5' #short name used for data_storage\n",
    "storage_path='../data/'\n",
    "unprocessed_path=f'{collection_short_name}_regular_grid/'\n",
    "zarr_file=f'{collection_short_name}_healpix.zarr'\n",
    "zarr_append_dim='time' #in the current state of this script only time makes sense and is the only dimension tested.\n",
    "\n",
    "#===== regridding config =====#\n",
    "\n",
    "nsides = [8, 16]\n",
    "interpolation_method = 'linear' # methode for the interpolation to healpix (one of \"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"quintic\", \"polynomial\")"
   ],
   "id": "ea1bf57cb8e05ee8",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T13:29:41.096873Z",
     "start_time": "2026-01-21T13:29:41.094672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not Path(storage_path).exists():\n",
    "    Path(storage_path).mkdir()\n",
    "\n",
    "if not Path(storage_path + unprocessed_path).exists():\n",
    "    Path(storage_path + unprocessed_path).mkdir()\n"
   ],
   "id": "5178bd5f7aa85fd5",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T13:29:41.102305Z",
     "start_time": "2026-01-21T13:29:41.100458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from zarr.errors import ZarrUserWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ZarrUserWarning)"
   ],
   "id": "8f240a5662713a76",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T13:29:41.114142Z",
     "start_time": "2026-01-21T13:29:41.110326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This cell iterates over the time period defined in temporal_extend and creates a dict containing day month and year for each day in the interval. This will later be used for the main downloading loop\n",
    "start_date = datetime.strptime(temporal_extend[\"start\"], '%Y-%m-%d')\n",
    "end_date = datetime.strptime(temporal_extend[\"end\"], '%Y-%m-%d')\n",
    "\n",
    "time_chunks = []\n",
    "for delta in range((end_date - start_date).days + 1): # iterate over the date range to create a dict for each day in our time_chunks list. Later we can loop over those list entries to loop over the days\n",
    "    i_date = start_date + timedelta(days=delta)\n",
    "    time_chunks.append({\n",
    "        \"day\": i_date.day,\n",
    "        \"month\": i_date.month,\n",
    "        \"year\": i_date.year,\n",
    "        \"string\": i_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"timestamp\": np.datetime64(i_date)\n",
    "    })\n",
    "\n",
    "time_chunks[:3] # :3 needed to limit console size for bigger temporal extends"
   ],
   "id": "a8b46348c7b7f540",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'day': 1,\n",
       "  'month': 12,\n",
       "  'year': 2024,\n",
       "  'string': '2024-12-01',\n",
       "  'timestamp': np.datetime64('2024-12-01T00:00:00.000000')},\n",
       " {'day': 2,\n",
       "  'month': 12,\n",
       "  'year': 2024,\n",
       "  'string': '2024-12-02',\n",
       "  'timestamp': np.datetime64('2024-12-02T00:00:00.000000')},\n",
       " {'day': 3,\n",
       "  'month': 12,\n",
       "  'year': 2024,\n",
       "  'string': '2024-12-03',\n",
       "  'timestamp': np.datetime64('2024-12-03T00:00:00.000000')}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T13:29:41.124752Z",
     "start_time": "2026-01-21T13:29:41.121991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extract the calculations for the healpix grids out of the main routine, so we don't do them multiple times (potentially for a lot of days / nside resolutions, amounting to a lot of calculations)\n",
    "hp_grids = []\n",
    "\n",
    "for nside in nsides: # iterate over nside resolutions and create the grid so we don't have to do it multiple tim\n",
    "    n_pix = hp.nside2npix(nside) # calculate number of pixels\n",
    "    theta, phi = hp.pix2ang(nside, np.arange(n_pix)) # calculate the pixel centers for each pixel in rad\n",
    "\n",
    "    hp_latitude = 90 - np.degrees(theta) # convert to degrees (North Pole = 0, South Pole = 180) and subtract from 90 to get to (North Pole = 90, South Pole = -90)\n",
    "    hp_longitude = np.degrees(phi) # convert to degrees (Greenwich = 0, increasing eastwards)\n",
    "\n",
    "    hp_grids.append({\"latitude\": hp_latitude, \"longitude\": hp_longitude, \"name\": f'nside-{nside}'})"
   ],
   "id": "6b09c553da7ed0d5",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T13:35:27.354647Z",
     "start_time": "2026-01-21T13:29:41.133693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "c = cdsapi.Client()\n",
    "\n",
    "zarr_path = storage_path + zarr_file\n",
    "\n",
    "for time_chunk in time_chunks:\n",
    "    file_name = f'{collection_short_name}_{time_chunk[\"string\"]}'\n",
    "    complete_path = storage_path + unprocessed_path + file_name + data_format[\"file_ending\"]\n",
    "\n",
    "    if Path(complete_path).exists():\n",
    "        print(f'{file_name + data_format[\"file_ending\"]} was already downloaded. Skipping download...')\n",
    "        try:\n",
    "            ds = xr.open_dataset(complete_path)\n",
    "        except:\n",
    "            print(f'the file {complete_path} appears to be corrupted. Please remove it and try again')\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Downloading {file_name + data_format[\"file_ending\"]}...\")\n",
    "        c.retrieve(collection, {\n",
    "            \"product_type\": ['reanalysis'],\n",
    "            \"variable\": variables,\n",
    "            \"year\": time_chunk[\"year\"],\n",
    "            \"month\": time_chunk[\"month\"],\n",
    "            \"day\": time_chunk[\"day\"],\n",
    "            \"time\": temporal_extend[\"times\"],\n",
    "            \"pressure_level\": pressure_levels,\n",
    "            \"grid\": grid,\n",
    "            \"area\": spacial_extend,\n",
    "           \"data_format\": data_format[\"format\"],\n",
    "        }, complete_path)\n",
    "        print(f\"Download completed\")\n",
    "        ds = xr.open_dataset(complete_path)\n",
    "\n",
    "    ds = ds.sortby(\"latitude\")\n",
    "\n",
    "    zarr_root = zarr.open_group(zarr_path, mode=\"a\") if Path(zarr_path).exists() else None #Only read zarr_root once per processed day -> no unnecessary metadata reads\n",
    "\n",
    "    for hp_grid in hp_grids:\n",
    "        group_path = hp_grid[\"name\"]\n",
    "        ds_existing = None\n",
    "        if zarr_root is not None and group_path in zarr_root.group_keys():\n",
    "            ds_existing = xr.open_zarr(zarr_path, group=group_path)\n",
    "\n",
    "        dimension_exists = ds_existing is not None and zarr_append_dim in ds_existing.dim and ds_existing.sizes[zarr_append_dim] > 0\n",
    "\n",
    "        if (\n",
    "            dimension_exists\n",
    "            and time_chunk[\"timestamp\"] in ds_existing[zarr_append_dim].to_index()\n",
    "        ):\n",
    "            print(f'{file_name} in resolution {hp_grid[\"name\"]} already exists. Skipping interpolation...')\n",
    "            continue\n",
    "\n",
    "        print(f\"Interpolating {file_name} in resolution {hp_grid[\"name\"]}\")\n",
    "\n",
    "        ds_hp = ds.interp(\n",
    "            latitude=xr.DataArray(hp_grid[\"latitude\"], dims='pix'),\n",
    "            longitude=xr.DataArray(hp_grid[\"longitude\"], dims='pix'),\n",
    "            method=interpolation_method\n",
    "        )\n",
    "\n",
    "        ds_hp = ds_hp.rename({\"valid_time\": 'time', \"pressure_level\": 'level'}) # rename to be more tsar_standard\n",
    "        ds_hp = ds_hp.assign_coords(pix=(\"pix\", np.arange(ds_hp.sizes[\"pix\"], dtype=np.int64)))\n",
    "        ds_hp = ds_hp.assign_coords(level=ds_hp[\"level\"].astype(np.int32))\n",
    "        ds_hp = ds_hp.drop_vars([\"expver\", \"number\"], errors=\"ignore\") #drop unnecessary variables\n",
    "\n",
    "\n",
    "        if (\n",
    "            dimension_exists\n",
    "        ):\n",
    "            print(f'Group {group_path} of archive {zarr_file} is not empty. Appending...')\n",
    "            ds_hp.to_zarr(zarr_path, group=group_path, mode=\"a\", append_dim=zarr_append_dim)\n",
    "            continue\n",
    "\n",
    "        print(f'Group {group_path} of archive {zarr_file} does not exists or is empty. Creating / Writing...')\n",
    "        ds_hp.to_zarr(zarr_path, group=group_path, mode=\"w\")\n",
    "\n",
    "    print(f'Processing of {collection_short_name}_{time_chunk[\"string\"]} completed.')"
   ],
   "id": "2eb425472bbda376",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era5_2024-12-01.nc was already downloaded. Skipping download...\n",
      "Interpolating era5_2024-12-01 in resolution nside-8\n",
      "Group nside-8 of archive era5_healpix.zarr doesn't exists or is empty. Creating / Writing...\n",
      "Interpolating era5_2024-12-01 in resolution nside-16\n",
      "Group nside-16 of archive era5_healpix.zarr doesn't exists or is empty. Creating / Writing...\n",
      "Processing of era5_2024-12-01 completed.\n",
      "era5_2024-12-02.nc was already downloaded. Skipping download...\n",
      "Interpolating era5_2024-12-02 in resolution nside-8\n",
      "Group nside-8 of archive era5_healpix.zarr not empty. Appending...\n",
      "Interpolating era5_2024-12-02 in resolution nside-16\n",
      "Group nside-16 of archive era5_healpix.zarr not empty. Appending...\n",
      "Processing of era5_2024-12-02 completed.\n",
      "Downloading era5_2024-12-03.nc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 14:29:44,076 INFO Request ID is 792eca74-66f8-4e57-ac5e-d0c8b886f294\n",
      "2026-01-21 14:29:44,384 INFO status has been updated to accepted\n",
      "2026-01-21 14:30:18,023 INFO status has been updated to successful\n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed\n",
      "Interpolating era5_2024-12-03 in resolution nside-8\n",
      "Group nside-8 of archive era5_healpix.zarr not empty. Appending...\n",
      "Interpolating era5_2024-12-03 in resolution nside-16\n",
      "Group nside-16 of archive era5_healpix.zarr not empty. Appending...\n",
      "Processing of era5_2024-12-03 completed.\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
