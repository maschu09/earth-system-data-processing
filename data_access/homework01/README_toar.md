Nils Hornstein, 7369566

**Identification of a Dataset**

To identify a specific dataset to work with, I went with the tip and picked air quality as a broad topic to focus on. In this context, I selected a specific variable later on. I started by organizing the list of given suggestions. Therefore, I created a table containing all datasets as entries and evaluated them regarding the type of data, the resolution and scope, ease of use and accessibility, and if applicable special features as well as the areas of application. To support this step and speed up the process, I designed a suitable LLM prompt: 

“Erstelle eine Vergleichstabelle für Wissenschaftler zum Thema Wetter-, Klima- und Erdbeobachtungsdaten. Die Tabelle soll den späteren Auswahlprozess eines Datensatzes unterstützen. Tabellenspalten: Art der Daten, Auflösung/Umfang (zeitlich und räumlich mit Einheiten), Benutzerfreundlichkeit/Zugänglichkeit (Begründung und vergebe eine Note schlecht, mittel, gut oder sehr gut), Besonderheiten/Einsatzbereiche. Bleib bei der Evaluation der einzelnen Datensätze kurz und präzise. Nutze die folgenden Datenquellen: [Namen + Links].”

Based on the table that ChatGPT-5 created using the prompt, I have selected two datasets that I find interesting regarding the overarching topic of air quality - the TOAR as well as the IAGOS dataset. In the end, I opted for the TOAR data on global air quality for several reasons.
First, TOAR appeared substantially more manageable for the scope of this assignment. While exploring both options, I felt somewhat overwhelmed by the complexity and breadth of the IAGOS dataset. In contrast, the TOAR data is supported by very clear, fairly well-structured documentation that not only explains how to access the data but also provides sample code snippets in Python and even a dedicated GitLab repository from a user workshop containing additional Jupyter notebooks.
Moreover, gaining access to TOAR data was considerably easier. Access was possible directly via Shibboleth using my University of Cologne account, which made the process straightforward and quick. The IAGOS dataset, on the other hand, required a much more involved registration workflow. This includes the creation of an additional AERIS account, completing a questionnaire, and submitting a request that needed approval. In summary, this made the IAGOS dataset less convenient and appealing. The clearer documentation, simpler access workflow, and overall lower complexity led me to choose TOAR as the more suitable dataset for this first assignment.

**Description of the TOAR Dataset**

The TOAR dataset is part of the TOAR database which is a central data repository for global data from surface ozone and ozone precursor measurements. Thereby, it is one of the largest collections of ozone-related surface observations worldwide. It is fully committed to Open Data and FAIR principles, and all data are provided without restrictions under a CC-BY 4.0 license. The term TOAR is an acronym standing for Tropospheric Ozone Assessment Report. Started as a project to support scientists worldwide to perform standardized analyses of ozone-related data, already the second phase referred as TOAR-II has started to further develop, extend and improve the TOAR database. The TOAR database is operated by the Jülich Supercomputing Centre at Forschungszentrum Jülich in Germany. 

Overall, the database contains measurements from almost 24,000 stations, covering a period from the 1970s up to 2022/2023. Its total data volume amounts to nearly 10 Terabytes. The dataset primarily consists of hourly surface-level ozone measurements, although finer temporal resolutions (e.g., half-hourly) are included when available, depending on the data provider. In addition, TOAR also offers aggregated statistics and trend estimates derived from these time series, all of which are based on the harmonised hourly format used for TOAR analyses to support long-term environmental assessments. Moreover, the database includes time series from ERA5 reanalysis that support the investigation of ozone changes and their drivers by providing relevant meteorological context. 

The data of the TOAR database originate from 18 major air-quality monitoring networks, public data services and many individual data providers. To ensure good data quality, only measurements from research-grade instruments are accepted. Besides this restriction, quality assurance is performed through a combination of provider-level checks, automated quality-control tools, and, in some cases, manual inspection. 
As the TOAR data is used for scientific papers during TOAR-II, insights from preliminary analyses of the data are also fed back into the database as a recursive feedback-loop. The TOAR database's data flagging scheme also makes it possible to distinguish the respective quality flags and identify the origin of this evaluation.

Beyond its main purpose of provisioning ground-level ozone concentration time series, one of its key features is the inclusion of harmonised metadata such as several ozone precursor variables and meteorological information. 
TOAR augments station metadata using globally uniform geospatial aggregates (GEO-PEAS), including population, landcover or nightlight statistics within user-defined radii. This geospatial metadata forms the basis to enable harmonized filtering and several station characterisation approaches. TOAR offers different options of characterisation for the measurement stations which differ in levels of complexity and harmonisation. Besides the location, this includes four distinct approaches for characterisation. Starting off with the TOAR Station Characterisation, the European Station Characterisation Scheme, the Station Characterisation Through Geospatial Data, as well as Individual Station Description. 
For example, the TOAR Station Characterisation distinguishes into four categories: Urban, RuralLowElevation, RuralHighElevation, and Unclassified. 

In terms of the data access, users don’t directly access the TOAR database itself. Instead, two access routes are provided. It can either be accessed through a graphical user interface, which they call dashboard, on the TOAR homepage, or through the REST API which can then be used in the browser or your own programs TOAR also distinguishes between the three user types anonymous, logged-in users, and registered TOAR user when it comes to data access. These groups differ in functionality, including the amount of data that can be downloaded at a time, per month, the usage of access tokens, and the level of access to various services. For example, anonymous users only have limited access, while logged-in as well as registered TOAR users can perform queries of greater size or use access tokens for the REST API.

The main part of the TOAR database is structured around stations and time series. Each station contains metadata (e.g., location, characterisation, geospatial aggregates) and a list of time series associated with it. But stations do not contain any measurement data themselves. These are stored in the time series. Each time series contains recorded values of one atmospheric variable from a specific station where it was measured. This can result in 1-to-m relationships, since multiple time series can exist for the same variable at a single station. For example, when different measurement techniques are used. However, a time series can only belong to one station. 

TOAR provides different services like search, data, and analysis to interact with it. The search service allows querying available stations and time series. The data service provides access to measurement data, including downloading in JSON, CSV, or HTML formats. The analysis service provides information for a specific time series for a given time stamp  and downloads them as ZIP folders.

Sources:
https://toar-data.org/about-toar/
https://toar-data.org/about-toar-data/#about_data
https://online.ucpress.edu/elementa/article/doi/10.1525/elementa.244/112447/Tropospheric-Ozone-Assessment-Report-Database-and
https://toar-data.fz-juelich.de/sphinx/TOAR_TG_Vol02_Data_Processing/build/html/processing-workflow.html?utm_source=chatgpt.com
https://toar-data.fz-juelich.de/sphinx/TOAR_UG_Vol03_Database/build/html/data-access.html#accessing-data-through-the-rest-application-programming-interface
https://toar-data.fz-juelich.de/sphinx/TOAR_UG_Vol04_FAQ/build/html/TOAR_database.html 
https://esde.pages.jsc.fz-juelich.de/toar-data/toardb_fastapi/docs/toardb_fastapi.html#data-flag
https://gitlab.jsc.fz-juelich.de/esde/toar-public/toar-data-user-workshop-2023
https://toar-data.fz-juelich.de/api/v2/#stationmeta
Schröder, S., Selke, N., and Schultz, M. G.: The TOAR data infrastructure: A generalised database infrastructure for environmental time series, EGU General Assembly 2023, Vienna, Austria, 24–28 Apr 2023, EGU23-1848, https://doi.org/10.5194/egusphere-egu23-1848, 2023 

**Data Access and Development of Download Script**

This section is intended to explain how data access works with the TOAR database. I will also discuss the development of the download script.
As already mentioned before, accessing data from the TOAR database primarily relies on requests to the REST API, which are structured around a base URL ‘https://toar-data-dev.fz-juelich.de/api/v2’. To query the database, users append the service they want to use, such as ‘stationmeta’, ‘timeseries’, or ‘search’, to this base URL. The query is then further refined using arguments, which must always be provided in the form ‘argument=value’. Thereby, the first argument has to be introduced by a ‘?’, and multiple arguments are concatenated using ‘&’. The resulting URL essentially acts as a direct query to the database.
Each service has a set of legal arguments, called query-options, which determine what can be controlled in a query. If certain arguments are omitted, the database applies default values, such as using JSON as the default output format. 
The documentation provides an overview of which query options can be used for which service. There is also a controlled vocabulary that can be used to find out which values the individual arguments can take.
To develop a download script, I started by playing around with the provided example codes from the documentation and expanding it with new functionalities. Even though the TOAR documentation is fairly generous, it took me some time to find where to get the information I needed. Sometimes the documentation felt a little bit scattered. This was especially in the case during the development of the download script when I tried writing different queries for the database. What I also discovered too late in this regard is the option to generate queries via the dashboard and its GUI.
Initially, I focused on the timeseries and station endpoints to retrieve data. However, I encountered challenges when trying to filter time series according to specific smaller ranges, like one week. To overcome this, I switched to the analysis services, which provide more refined search capabilities. In order to be able to actually further process the data obtained in this way, it first had to be extracted from its form as a collected ZIP folder. In the end, the time series is broken down into smaller parts so that each day is stored in a single file, either as csv or JSON. I have implemented both variants and I see advantages for both use cases. For example, csv files are clearer arranged and thus a lot easier to process as human.
However, I did not abandon the data query via the time series endpoint and continued to pursue it. Initially, I had the problem of filtering smaller time periods from a large time series. However, as I became more familiar with the TOAR database and the documentation, I came up with an approach. This requires that a suitable time series be found first via consecutive queries. This step is not necessary with Analysis Service due to its different structure. The selected time series is then downloaded via the time series endpoint, temporarily stored in a Pandas data frame, and only saved in CSV files after filtering for a desired sub-period in a structure similar to that used by the analysis service. The final download script contains all of the three variants listed here.
As mentioned before, TOAR distinguishes between several user types. In my case, I stick with the logged-in user. Unfortunately, I didn’t pay enough attention to the download limitations as a logged-in user. The limit here is 250 files per month. As it happened only one day before submission, I was unable to apply for access as a registered TOAR user in time. I therefore hope that the code will continue to work even after the minor adjustments, mainly relating to clean-ups. Unfortunately, I was unable to test this conclusively.

**Scalability and Code Improvement**

The code could definitely be improved in terms of readability, structure, and clarity. One key enhancement would be to modularize the script by moving distinct functionalities into separate methods or functions that are then called from the main script. This would make the code easier to maintain, debug, and extend. It also seems a little more complicated than necessary in some places. 
Looking back, the initial idea of implementing several strategies at once made the script unnecessarily complex. A more effective approach would have been to commit to one clear strategy first and work it out fully before considering alternatives.
When thinking about scaling data access, it would be necessary to take a closer look at the folder structure used to store downloaded data. There is a clear need for more sorting and for a hierarchical level dedicated to time series, since multiple time series may exist for a given station and variable. A structure such as Station > Variable > Time series >… > Files could be suitable. However, adopting such a structure would require a broader overhaul of the script, as it currently does not account for the possibility of multiple time series.
The script is also lacking a good way for users to easily customize which data they want to download. I could imagine that the appropriate query could be created via the TOAR dashboard GUI and then only need to be specified by the user together with the access token.
Furthermore, several approaches that are important for good scalability, many of which were covered in class, have not yet been implemented in the script. This includes, for example, the use of flags. At the moment, the script offers little transparency regarding which data has been downloaded and whether the retrieved data is complete.
I can imagine that continuing to build on the approach using the Analysis Service would be beneficial, especially since the data is already provided in compressed form. This could be a major advantage when handling large datasets.
